[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Intermediate Research Methods and Statistics",
    "section": "",
    "text": "By Usman Afzali, PhD"
  },
  {
    "objectID": "labs/rstudio-and-quarto/index.html",
    "href": "labs/rstudio-and-quarto/index.html",
    "title": "Lab 5: Hypothesis testing with t-tests",
    "section": "",
    "text": "In this lab, you will be conducting t-tests with Jamovi and practice reporting results in APA-style format. As you heard in the lectures, we will always encounter violations of assumptions when dealing with real-life data. This lab exercise has many such instances and we will practice how to choose from non-parametric tests when assumptions are violated. We will also be reporting effect sizes. Note the interpretation guidelines for “Cohen’s d” and for “Rank biserial correlation” (0.2 = ‘small’; 0.5 = ‘medium’; 0.8 = ‘large’). This lab is important because you will learn how to choose a statistical analysis plan based on the research question, how variables were measured, and the characteristics of the dataset. These are important skills that will be assessed in your Lab Report."
  },
  {
    "objectID": "labs/rstudio-and-quarto/index.html#task-1-living-with-partner-and-getting-on",
    "href": "labs/rstudio-and-quarto/index.html#task-1-living-with-partner-and-getting-on",
    "title": "Lab 5: Hypothesis testing with t-tests",
    "section": "Task 1: Living with partner and getting on",
    "text": "Task 1: Living with partner and getting on\nDid people who lived with their partner get on better or worse with their cohabitant(s) over lockdown than people who did not live with a partner?\nQ1. Open the dataset that we used in one of the previous labs (you can download the ‘Lab 5 dataset.xlsx’ file from the lab section on Learn). Read through the codebook to get an understanding of the survey questions in the dataset. Let’s have a look at the structure too.\n\n\nCode\nlibrary(readxl)\ndf <- read_xlsx(\"Lab 5 Dataset.xlsx\")\nstr(df)\n\n\ntibble [140 × 14] (S3: tbl_df/tbl/data.frame)\n $ RESP_ID  : chr [1:140] \"Response ID\" \"1\" \"2\" \"3\" ...\n $ COV_5a   : chr [1:140] \"Lived with: My partner\" \"0\" \"1\" \"1\" ...\n $ COV_5b   : chr [1:140] \"Lived with: My child(ren)\" \"0\" \"0\" \"0\" ...\n $ COV_5c   : chr [1:140] \"Lived with: My parent(s)\" \"1\" \"1\" \"1\" ...\n $ COV_5d   : chr [1:140] \"Lived with: My sibling(s)\" \"0\" \"0\" \"0\" ...\n $ COV_5e   : chr [1:140] \"Lived with: Other people related to me (eg extended family)\" \"0\" \"1\" \"0\" ...\n $ COV_5f   : chr [1:140] \"Lived with: Other people not related to me (eg flatmates)\" \"0\" \"0\" \"1\" ...\n $ COV_5g   : chr [1:140] \"Lived with: hall of residence or hostel\" \"0\" \"0\" \"0\" ...\n $ COV_5h   : chr [1:140] \"Lived with: I lived alone\" \"0\" \"0\" \"0\" ...\n $ COV_6    : chr [1:140] \"Were your living arrangements during lockdown (in terms of who you lived with):\" \"Different\" \"Different\" \"Different\" ...\n $ COV_7    : chr [1:140] \"Thinking about the people you lived with during lockdown, on a scale of 1-10, how well do you feel you got alon\"| __truncated__ \"9.1999999999999993\" \"7.1\" \"4\" ...\n $ COV_15   : chr [1:140] \"In general, how strongly do you agree or disagree with the government's decision to implement Level 4 lockdown in New Zealand?\" \"5\" \"5\" \"5\" ...\n $ Age_coded: chr [1:140] \"Age group\" \"19-24\" \"19-24\" \"19-24\" ...\n $ Gender   : chr [1:140] \"What gender do you identify with?\" \"Male\" \"Female\" \"Female\" ...\n\n\nWe need to remove the first row.\n\n\nCode\ncol <- 1\ndf <- df[-c(1), ]\nstr(df)\n\n\ntibble [139 × 14] (S3: tbl_df/tbl/data.frame)\n $ RESP_ID  : chr [1:139] \"1\" \"2\" \"3\" \"4\" ...\n $ COV_5a   : chr [1:139] \"0\" \"1\" \"1\" \"1\" ...\n $ COV_5b   : chr [1:139] \"0\" \"0\" \"0\" \"0\" ...\n $ COV_5c   : chr [1:139] \"1\" \"1\" \"1\" \"0\" ...\n $ COV_5d   : chr [1:139] \"0\" \"0\" \"0\" \"0\" ...\n $ COV_5e   : chr [1:139] \"0\" \"1\" \"0\" \"0\" ...\n $ COV_5f   : chr [1:139] \"0\" \"0\" \"1\" \"1\" ...\n $ COV_5g   : chr [1:139] \"0\" \"0\" \"0\" \"0\" ...\n $ COV_5h   : chr [1:139] \"0\" \"0\" \"0\" \"0\" ...\n $ COV_6    : chr [1:139] \"Different\" \"Different\" \"Different\" \"Different\" ...\n $ COV_7    : chr [1:139] \"9.1999999999999993\" \"7.1\" \"4\" \"9.3000000000000007\" ...\n $ COV_15   : chr [1:139] \"5\" \"5\" \"5\" \"5\" ...\n $ Age_coded: chr [1:139] \"19-24\" \"19-24\" \"19-24\" \"19-24\" ...\n $ Gender   : chr [1:139] \"Male\" \"Female\" \"Female\" \"Female\" ...\n\n\nQ2. First we are going to investigate whether people who lived with their partner get on better or worse with their cohabitant(s) over lockdown than people who did not live with a partner. From your understanding of the codebook, what is your independent variable and what is your dependent variable?\nQ3. What type of analysis is appropriate given these variables?\nHINT: Use your statistics decision-making tree and your codebook to help you with this question.\nBetter to rename our variables and also make sure the variable types are appropriate for analysis.\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.4 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.0      ✔ stringr 1.4.0 \n✔ readr   2.1.2      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\ndf <- df %>%\n  mutate(gettingOn = as.numeric(COV_7)) %>%\n  mutate(livedPartner = as.character(COV_5a))\n\n\nLet’s get a boxplot to make sure\n\n\nCode\nboxplot(gettingOn ~ livedPartner, data = df)\n\n\n\n\n\n\n\n\n\nGenerally, we would go for a t-test for independent means.\n\n\nCode\nt.test(gettingOn ~ livedPartner, alt = \"two.sided\", conf = 0.95, var.eq = T, data = df)\n\n\n\n    Two Sample t-test\n\ndata:  gettingOn by livedPartner\nt = -2.2637, df = 134, p-value = 0.0252\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -1.5461757 -0.1042031\nsample estimates:\nmean in group 0 mean in group 1 \n       7.547727        8.372917 \n\n\nQ4. Test the assumptions of homogeneity and normality. What is the conclusion of your assumption tests? What does this mean for hypothesis testing?\nNOTE: Violation of normality can be better understood graphically—if you run descriptives and create histograms graphs of your study variables, you can see more easily that the curve is not normal.\nLet’s look at normality first.\n\n\nCode\nshapiro.test (df$gettingOn)\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  df$gettingOn\nW = 0.87295, p-value = 1.978e-09\n\n\nNow, to homogneity of variance.\n\n\nCode\ncar::leveneTest(gettingOn ~ livedPartner, df)\n\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value Pr(>F)\ngroup   1  1.9741 0.1623\n      134               \n\n\nAs we can see, the assumption of normality is violated. So, let’s conduct a Mann-Whitney U test instead.\n\n\nCode\nwilcox.test(gettingOn ~ livedPartner, df)\n\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  gettingOn by livedPartner\nW = 1547, p-value = 0.009829\nalternative hypothesis: true location shift is not equal to 0\n\n\nQ5. In the ‘Additional statistics’, select ‘Effect size’, ‘Descriptives’, and ‘Descriptives plots’. Report your NHST results and interpret your findings:\n\nMean or Median scores for each group on the dependent variable\nThe appropriate statistic (t or otherwise)\nSignificance (or otherwise) of the test\nEffect size\n\nNOTE: A measure of effect size for a t-test is the Cohen’s d. A measure of effect size for a non-parametric test is the rank biserial correlation.\nLet’s do the rest of it with jamovi. The outcome is a bit different because they might be using different packages.\n\n\nCode\njmv::ttestIS(\n  formula = gettingOn ~ COV_5a,\n  data = df,\n  vars = gettingOn,\n  students = FALSE,\n  mann = TRUE,\n  norm = TRUE,\n  eqv = TRUE,\n  effectSize = TRUE,\n  desc = TRUE)\n\n\n\n INDEPENDENT SAMPLES T-TEST\n\n Independent Samples T-Test                                                                            \n ───────────────────────────────────────────────────────────────────────────────────────────────────── \n                                  Statistic    p                                         Effect Size   \n ───────────────────────────────────────────────────────────────────────────────────────────────────── \n   gettingOn    Mann-Whitney U     1547.000    0.0098285    Rank biserial correlation      0.2675189   \n ───────────────────────────────────────────────────────────────────────────────────────────────────── \n\n\n ASSUMPTIONS\n\n Normality Test (Shapiro-Wilk)            \n ──────────────────────────────────────── \n                W            p            \n ──────────────────────────────────────── \n   gettingOn    0.8915312    < .0000001   \n ──────────────────────────────────────── \n   Note. A low p-value suggests a\n   violation of the assumption of\n   normality\n\n\n Homogeneity of Variances Test (Levene's)            \n ─────────────────────────────────────────────────── \n                F           df    df2    p           \n ─────────────────────────────────────────────────── \n   gettingOn    2.634183     1    134    0.1069362   \n ─────────────────────────────────────────────────── \n   Note. A low p-value suggests a violation of\n   the assumption of equal variances\n\n\n Group Descriptives                                                            \n ───────────────────────────────────────────────────────────────────────────── \n                Group    N     Mean        Median      SD          SE          \n ───────────────────────────────────────────────────────────────────────────── \n   gettingOn    0        88    7.547727    8.100000    2.113230    0.2252711   \n                1        48    8.372917    9.100000    1.871027    0.2700594   \n ─────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "labs/rstudio-and-quarto/index.html#task-2-change-in-living-arrnagement-and-getting-on",
    "href": "labs/rstudio-and-quarto/index.html#task-2-change-in-living-arrnagement-and-getting-on",
    "title": "Lab 5: Hypothesis testing with t-tests",
    "section": "Task 2: Change in living arrnagement and getting on",
    "text": "Task 2: Change in living arrnagement and getting on\nDid people whose living arrangements changed just prior to lockdown get on better or worse with their cohabitant(s) than those whose living arrangements stayed the same?\nQ6. Now we are going to investigate whether people whose living arrangements changed just prior to lockdown get on better or worse with their cohabitant(s) than those whose living arrangements stayed the same. From your understanding of the codebook, what is your independent variable and what is your dependent variable?\nQ7. What type of analysis is appropriate given these variables?\nRenaming\n\n\nCode\nlibrary(tidyverse)\ndf <- df %>%\n  mutate(changedCond = as.factor(COV_6))\n\n\nNow, the t-test\n\n\nCode\nt.test(gettingOn ~ changedCond, alt = \"two.sided\", conf = 0.95, var.eq = T, data = df)\n\n\n\n    Two Sample t-test\n\ndata:  gettingOn by changedCond\nt = -0.28567, df = 133, p-value = 0.7756\nalternative hypothesis: true difference in means between group Different and group Same is not equal to 0\n95 percent confidence interval:\n -0.8318547  0.6218934\nsample estimates:\nmean in group Different      mean in group Same \n               7.806383                7.911364 \n\n\nQ8. In ‘Assumption checks’, select ‘Homogeneity test’ and ‘Normality test’. What is the conclusion of your assumption tests? What does this mean for hypothesis testing?\nQ9. In the ‘Additional statistics’, select ‘Effect size’, ‘Descriptives’, and ‘Descriptives plots’. Report your NHST results and interpret your findings:\n\nMean or Median scores for each group on the dependent variable\nThe appropriate statistic (t or otherwise)\nSignificance (or otherwise) of the test\nEffect size\n\nAssumptions: We already know that normality is violated.\n\n\nCode\nshapiro.test (df$gettingOn)\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  df$gettingOn\nW = 0.87295, p-value = 1.978e-09\n\n\nHomogeneity of variance\n\n\nCode\ncar::leveneTest(gettingOn ~ changedCond, df)\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value Pr(>F)\ngroup   1  0.9051 0.3431\n      133               \n\n\nMann-Whitney U test\n\n\nCode\nwilcox.test(gettingOn ~ changedCond, df)\n\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  gettingOn by changedCond\nW = 1917.5, p-value = 0.4865\nalternative hypothesis: true location shift is not equal to 0\n\n\nOr, with code from jamovi.\n\n\nCode\njmv::ttestIS(\n  formula = gettingOn ~ COV_5a,\n  data = df,\n  vars = gettingOn,\n  students = FALSE,\n  mann = TRUE,\n  norm = TRUE,\n  eqv = TRUE,\n  effectSize = TRUE,\n  desc = TRUE)\n\n\n\n INDEPENDENT SAMPLES T-TEST\n\n Independent Samples T-Test                                                                            \n ───────────────────────────────────────────────────────────────────────────────────────────────────── \n                                  Statistic    p                                         Effect Size   \n ───────────────────────────────────────────────────────────────────────────────────────────────────── \n   gettingOn    Mann-Whitney U     1547.000    0.0098285    Rank biserial correlation      0.2675189   \n ───────────────────────────────────────────────────────────────────────────────────────────────────── \n\n\n ASSUMPTIONS\n\n Normality Test (Shapiro-Wilk)            \n ──────────────────────────────────────── \n                W            p            \n ──────────────────────────────────────── \n   gettingOn    0.8915312    < .0000001   \n ──────────────────────────────────────── \n   Note. A low p-value suggests a\n   violation of the assumption of\n   normality\n\n\n Homogeneity of Variances Test (Levene's)            \n ─────────────────────────────────────────────────── \n                F           df    df2    p           \n ─────────────────────────────────────────────────── \n   gettingOn    2.634183     1    134    0.1069362   \n ─────────────────────────────────────────────────── \n   Note. A low p-value suggests a violation of\n   the assumption of equal variances\n\n\n Group Descriptives                                                            \n ───────────────────────────────────────────────────────────────────────────── \n                Group    N     Mean        Median      SD          SE          \n ───────────────────────────────────────────────────────────────────────────── \n   gettingOn    0        88    7.547727    8.100000    2.113230    0.2252711   \n                1        48    8.372917    9.100000    1.871027    0.2700594   \n ─────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "labs/rstudio-and-quarto/index.html#task-3-male-and-female-vs.-govt-decision",
    "href": "labs/rstudio-and-quarto/index.html#task-3-male-and-female-vs.-govt-decision",
    "title": "Lab 5: Hypothesis testing with t-tests",
    "section": "Task 3: Male and female vs. govt decision",
    "text": "Task 3: Male and female vs. govt decision\nDid males and females differ with respect to their level of agreement with the government’s decision to implement the lockdown?\nQ10. Now we’re going to see whether males and females differed with respect to their level of agreement with the government’s decision to implement the lockdown. From your understanding of the codebook, what is your independent variable and what is your dependent variable?\nQ11. What type of analysis is appropriate given these variables?\nRenaming\n\n\nCode\nlibrary(tidyverse)\ndf <- df %>% \n  mutate(govDecision = as.numeric(COV_15))\n\n\nNow, the t-test\n\n\nCode\nt.test(govDecision ~ Gender, alt = \"two.sided\", conf = 0.95, var.eq = T, data = df)\n\n\n\n    Two Sample t-test\n\ndata:  govDecision by Gender\nt = 3.2589, df = 135, p-value = 0.001415\nalternative hypothesis: true difference in means between group Female and group Male is not equal to 0\n95 percent confidence interval:\n 0.2062318 0.8429380\nsample estimates:\nmean in group Female   mean in group Male \n            4.731481             4.206897 \n\n\nQ12. Assumption checks. What is the conclusion of your assumption tests? What does this mean for hypothesis testing?\nAssumptions\n\n\nCode\nshapiro.test (df$govDecision)\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  df$govDecision\nW = 0.52955, p-value < 2.2e-16\n\n\nCode\ncar::leveneTest(govDecision ~ Gender, df)\n\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value   Pr(>F)   \ngroup   1   10.62 0.001415 **\n      135                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nMann-Whitney U test\n\n\nCode\nwilcox.test(govDecision ~ Gender, df)\n\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  govDecision by Gender\nW = 1919, p-value = 0.01477\nalternative hypothesis: true location shift is not equal to 0\n\n\nQ13. In the ‘Additional statistics’, select ‘Effect size’, ‘Descriptives’, and ‘Descriptives plots’. Report your NHST results and interpret your findings:\n\nMean or Median scores for each group on the dependent variable\nThe appropriate statistic (t or otherwise)\nSignificance (or otherwise) of the test\nEffect size\n\n\n\nCode\njmv::ttestIS(\n  formula = govDecision ~ Gender,\n  data = df,\n  vars = govDecision,\n  students = FALSE,\n  mann = TRUE,\n  norm = TRUE,\n  eqv = TRUE,\n  effectSize = TRUE,\n  desc = TRUE)\n\n\n\n INDEPENDENT SAMPLES T-TEST\n\n Independent Samples T-Test                                                                              \n ─────────────────────────────────────────────────────────────────────────────────────────────────────── \n                                    Statistic    p                                         Effect Size   \n ─────────────────────────────────────────────────────────────────────────────────────────────────────── \n   govDecision    Mann-Whitney U     1213.000    0.0147729    Rank biserial correlation      0.2254151   \n ─────────────────────────────────────────────────────────────────────────────────────────────────────── \n\n\n ASSUMPTIONS\n\n Normality Test (Shapiro-Wilk)              \n ────────────────────────────────────────── \n                  W            p            \n ────────────────────────────────────────── \n   govDecision    0.6755793    < .0000001   \n ────────────────────────────────────────── \n   Note. A low p-value suggests a\n   violation of the assumption of\n   normality\n\n\n Homogeneity of Variances Test (Levene's)              \n ───────────────────────────────────────────────────── \n                  F           df    df2    p           \n ───────────────────────────────────────────────────── \n   govDecision    22.29761     1    135    0.0000058   \n ───────────────────────────────────────────────────── \n   Note. A low p-value suggests a violation of the\n   assumption of equal variances\n\n\n Group Descriptives                                                                  \n ─────────────────────────────────────────────────────────────────────────────────── \n                  Group     N      Mean        Median      SD           SE           \n ─────────────────────────────────────────────────────────────────────────────────── \n   govDecision    Female    108    4.731481    5.000000    0.5897327    0.05674706   \n                  Male       29    4.206897    5.000000     1.235756     0.2294742   \n ───────────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "labs/why-quarto/index.html",
    "href": "labs/why-quarto/index.html",
    "title": "Lab 3 - Descriptive Statisitcs",
    "section": "",
    "text": "In this lab, you will learn how to clean up a dataset to prepare it for analyses.\n\n\nWe will use a provided dataset for this task. Note that this file is still cleaned up to some extent. When you download a completed survey from Qualtrics, it will have many additional columns that are mostly not needed for data analysis and we will not normally use them.\nQ1. Download the Lab 03 Dataset.xlsx and Lab 03 Codebook.xlsx files from Learn. The first file is the dataset (note that some small modifications have been made to protect respondents’ privacy). Responses have also been coded according to the codebook file—take a few minutes to read through the codebook so you understand this dataset.\n\ndf <- readxl::read_xlsx(\"Lab 03 Dataset.xlsx\")\ndf\n\n# A tibble: 140 × 30\n   RESP_ID     COV_1 COV_2 COV_3 COV_4 COV_9a COV_9b COV_9c COV_9d COV_9e COV_10\n   <chr>       <chr> <chr> <chr> <chr> <chr>  <chr>  <chr>  <chr>  <chr>  <chr> \n 1 Response ID Just… Just… Just… On a… Ease … Ease … Ease … Ease … Ease … How d…\n 2 1           2     2     2     8.30… 3      <NA>   4      <NA>   <NA>   4     \n 3 2           3     2     3     9.19… 3      <NA>   3      <NA>   <NA>   3     \n 4 3           2     2     2     5.7   5      5      4      4      <NA>   4     \n 5 4           1     1     2     7     2      1      4      3      <NA>   3     \n 6 5           1     1     2     5     3      <NA>   2      4      <NA>   5     \n 7 6           3     4     2     6.1   <NA>   <NA>   4      5      <NA>   2     \n 8 7           1     1     3     7.1   <NA>   <NA>   <NA>   <NA>   <NA>   4     \n 9 8           1     2     3     10    <NA>   <NA>   <NA>   <NA>   <NA>   3     \n10 9           4     4     4     8.30… 3      3      <NA>   <NA>   <NA>   4     \n# … with 130 more rows, and 19 more variables: FIN_1_1 <chr>, FIN_1_2 <chr>,\n#   FIN_1_3 <chr>, FIN_1_4 <chr>, FIN_1_5 <chr>, FIN_1_6 <chr>, FIN_1_7 <chr>,\n#   FIN_1_8 <chr>, FIN_2_1 <chr>, FIN_2_2 <chr>, FIN_2_3 <chr>, FIN_2_4 <chr>,\n#   FIN_2_5 <chr>, FIN_2_6 <chr>, FIN_2_7 <chr>, FIN_2_8 <chr>, FIN_2_9 <chr>,\n#   Age_coded <chr>, Gender <chr>\n\n\nHave a look at the structure\n\nstr(df)\n\ntibble [140 × 30] (S3: tbl_df/tbl/data.frame)\n $ RESP_ID  : chr [1:140] \"Response ID\" \"1\" \"2\" \"3\" ...\n $ COV_1    : chr [1:140] \"Just prior to lockdown, how worried were you about your likelihood of contracting COVID-19?\" \"2\" \"3\" \"2\" ...\n $ COV_2    : chr [1:140] \"Just prior to lockdown, how worried were you about suffering serious medical complications if you contracted COVID-19?\" \"2\" \"2\" \"2\" ...\n $ COV_3    : chr [1:140] \"Just prior to lockdown, how worried were you about passing COVID-19 on to someone else if you contracted it?\" \"2\" \"3\" \"2\" ...\n $ COV_4    : chr [1:140] \"On a scale of 1-10, how well would you say you coped overall during lockdown?\" \"8.3000000000000007\" \"9.1999999999999993\" \"5.7\" ...\n $ COV_9a   : chr [1:140] \"Ease of access: Academic support\" \"3\" \"3\" \"5\" ...\n $ COV_9b   : chr [1:140] \"Ease of access: Financial help\" NA NA \"5\" ...\n $ COV_9c   : chr [1:140] \"Ease of access: Health services\" \"4\" \"3\" \"4\" ...\n $ COV_9d   : chr [1:140] \"Ease of access: Mental health services\" NA NA \"4\" ...\n $ COV_9e   : chr [1:140] \"Ease of access: Other\" NA NA NA ...\n $ COV_10   : chr [1:140] \"How do you feel the pandemic impacted your academic performance in Semester 1?\" \"4\" \"3\" \"4\" ...\n $ FIN_1_1  : chr [1:140] \"I constantly worry about my financial situation\" \"3\" \"3\" \"5\" ...\n $ FIN_1_2  : chr [1:140] \"I try not to think about how much debt I am in\" \"4\" \"6\" \"2\" ...\n $ FIN_1_3  : chr [1:140] \"My income is sufficient to meet my needs\" \"5\" \"6\" \"3\" ...\n $ FIN_1_4  : chr [1:140] \"I think my financial position has a negative effect on my social life\" \"4\" \"5\" \"5\" ...\n $ FIN_1_5  : chr [1:140] \"I think my financial position has a negative effect on my study\" \"3\" \"2\" \"6\" ...\n $ FIN_1_6  : chr [1:140] \"Not meeting my weekly financial demands is constantly on my mind\" \"4\" \"3\" \"5\" ...\n $ FIN_1_7  : chr [1:140] \"Worrying about money affects my daily mood\" \"4\" \"5\" \"5\" ...\n $ FIN_1_8  : chr [1:140] \"I feel like I don‚Äôt have enough money to do the things I enjoy\" \"4\" \"5\" \"5\" ...\n $ FIN_2_1  : chr [1:140] \"I find myself stressing about upcoming payments\" \"5\" \"2\" \"5\" ...\n $ FIN_2_2  : chr [1:140] \"I feel stressed when I receive my bills\" \"4\" \"6\" \"5\" ...\n $ FIN_2_3  : chr [1:140] \"I am often concerned I will not have enough funds to make necessary purchases\" \"5\" \"2\" \"4\" ...\n $ FIN_2_4  : chr [1:140] \"I spend all my money on living costs\" \"4\" \"1\" \"3\" ...\n $ FIN_2_5  : chr [1:140] \"I regularly miss out on social occasions due to finances\" \"5\" \"4\" \"3\" ...\n $ FIN_2_6  : chr [1:140] \"I compromise my well-being due to my financial situation\" \"4\" \"4\" \"4\" ...\n $ FIN_2_7  : chr [1:140] \"I am able to easily balance my finances with my social life\" \"5\" \"3\" \"4\" ...\n $ FIN_2_8  : chr [1:140] \"Financial stress restricts my social life\" \"4\" \"5\" \"5\" ...\n $ FIN_2_9  : chr [1:140] \"I avoid interactions that involve money\" \"4\" \"6\" \"5\" ...\n $ Age_coded: chr [1:140] \"Age\" \"19-24\" \"19-24\" \"19-24\" ...\n $ Gender   : chr [1:140] \"What gender do you identify with?\" \"Male\" \"Female\" \"Female\" ...\n\n\nQ2. Open the dataset file with R. Discuss with your peer why you should delete the first row.\n\ndf <- df[-c(1), ]\n\nQ3. Use your codebook to determine if R has the correct measurement type (nominal, ordinal, or continuous) and data type (integer, decimal, text) and make changes if they are not correct.\nTo check the type of measure, we use `str’\n\nstr(df)\n\ntibble [139 × 30] (S3: tbl_df/tbl/data.frame)\n $ RESP_ID  : chr [1:139] \"1\" \"2\" \"3\" \"4\" ...\n $ COV_1    : chr [1:139] \"2\" \"3\" \"2\" \"1\" ...\n $ COV_2    : chr [1:139] \"2\" \"2\" \"2\" \"1\" ...\n $ COV_3    : chr [1:139] \"2\" \"3\" \"2\" \"2\" ...\n $ COV_4    : chr [1:139] \"8.3000000000000007\" \"9.1999999999999993\" \"5.7\" \"7\" ...\n $ COV_9a   : chr [1:139] \"3\" \"3\" \"5\" \"2\" ...\n $ COV_9b   : chr [1:139] NA NA \"5\" \"1\" ...\n $ COV_9c   : chr [1:139] \"4\" \"3\" \"4\" \"4\" ...\n $ COV_9d   : chr [1:139] NA NA \"4\" \"3\" ...\n $ COV_9e   : chr [1:139] NA NA NA NA ...\n $ COV_10   : chr [1:139] \"4\" \"3\" \"4\" \"3\" ...\n $ FIN_1_1  : chr [1:139] \"3\" \"3\" \"5\" \"5\" ...\n $ FIN_1_2  : chr [1:139] \"4\" \"6\" \"2\" \"6\" ...\n $ FIN_1_3  : chr [1:139] \"5\" \"6\" \"3\" \"7\" ...\n $ FIN_1_4  : chr [1:139] \"4\" \"5\" \"5\" \"1\" ...\n $ FIN_1_5  : chr [1:139] \"3\" \"2\" \"6\" \"5\" ...\n $ FIN_1_6  : chr [1:139] \"4\" \"3\" \"5\" \"5\" ...\n $ FIN_1_7  : chr [1:139] \"4\" \"5\" \"5\" \"1\" ...\n $ FIN_1_8  : chr [1:139] \"4\" \"5\" \"5\" \"1\" ...\n $ FIN_2_1  : chr [1:139] \"5\" \"2\" \"5\" \"3\" ...\n $ FIN_2_2  : chr [1:139] \"4\" \"6\" \"5\" \"2\" ...\n $ FIN_2_3  : chr [1:139] \"5\" \"2\" \"4\" \"4\" ...\n $ FIN_2_4  : chr [1:139] \"4\" \"1\" \"3\" \"1\" ...\n $ FIN_2_5  : chr [1:139] \"5\" \"4\" \"3\" \"1\" ...\n $ FIN_2_6  : chr [1:139] \"4\" \"4\" \"4\" \"1\" ...\n $ FIN_2_7  : chr [1:139] \"5\" \"3\" \"4\" \"6\" ...\n $ FIN_2_8  : chr [1:139] \"4\" \"5\" \"5\" \"1\" ...\n $ FIN_2_9  : chr [1:139] \"4\" \"6\" \"5\" \"1\" ...\n $ Age_coded: chr [1:139] \"19-24\" \"19-24\" \"19-24\" \"19-24\" ...\n $ Gender   : chr [1:139] \"Male\" \"Female\" \"Female\" \"Female\" ...\n\n\nWe can see that all continuous measures (cloums 2:27) that are supposed to be numeric, are string (character). We need to change them to numeric first.\n\ncols <- 2:28\ndf[cols] <- lapply(df[cols], as.numeric)\nstr(df)\n\ntibble [139 × 30] (S3: tbl_df/tbl/data.frame)\n $ RESP_ID  : chr [1:139] \"1\" \"2\" \"3\" \"4\" ...\n $ COV_1    : num [1:139] 2 3 2 1 1 3 1 1 4 4 ...\n $ COV_2    : num [1:139] 2 2 2 1 1 4 1 2 4 4 ...\n $ COV_3    : num [1:139] 2 3 2 2 2 2 3 3 4 5 ...\n $ COV_4    : num [1:139] 8.3 9.2 5.7 7 5 6.1 7.1 10 8.3 8.1 ...\n $ COV_9a   : num [1:139] 3 3 5 2 3 NA NA NA 3 2 ...\n $ COV_9b   : num [1:139] NA NA 5 1 NA NA NA NA 3 2 ...\n $ COV_9c   : num [1:139] 4 3 4 4 2 4 NA NA NA NA ...\n $ COV_9d   : num [1:139] NA NA 4 3 4 5 NA NA NA NA ...\n $ COV_9e   : num [1:139] NA NA NA NA NA NA NA NA NA NA ...\n $ COV_10   : num [1:139] 4 3 4 3 5 2 4 3 4 2 ...\n $ FIN_1_1  : num [1:139] 3 3 5 5 1 5 3 1 7 3 ...\n $ FIN_1_2  : num [1:139] 4 6 2 6 1 5 6 6 6 5 ...\n $ FIN_1_3  : num [1:139] 5 6 3 7 6 4 2 7 1 5 ...\n $ FIN_1_4  : num [1:139] 4 5 5 1 1 6 3 1 6 1 ...\n $ FIN_1_5  : num [1:139] 3 2 6 5 1 5 2 1 6 1 ...\n $ FIN_1_6  : num [1:139] 4 3 5 5 1 4 5 1 6 2 ...\n $ FIN_1_7  : num [1:139] 4 5 5 1 1 4 2 1 6 2 ...\n $ FIN_1_8  : num [1:139] 4 5 5 1 2 7 3 1 6 2 ...\n $ FIN_2_1  : num [1:139] 5 2 5 3 1 4 6 2 6 2 ...\n $ FIN_2_2  : num [1:139] 4 6 5 2 1 4 6 1 6 2 ...\n $ FIN_2_3  : num [1:139] 5 2 4 4 2 4 6 1 6 3 ...\n $ FIN_2_4  : num [1:139] 4 1 3 1 1 4 5 1 6 4 ...\n $ FIN_2_5  : num [1:139] 5 4 3 1 1 5 4 1 6 2 ...\n $ FIN_2_6  : num [1:139] 4 4 4 1 1 6 2 1 7 5 ...\n $ FIN_2_7  : num [1:139] 5 3 4 6 6 2 3 7 2 6 ...\n $ FIN_2_8  : num [1:139] 4 5 5 1 1 6 5 1 6 1 ...\n $ FIN_2_9  : num [1:139] 4 6 5 1 1 5 6 5 6 3 ...\n $ Age_coded: chr [1:139] \"19-24\" \"19-24\" \"19-24\" \"19-24\" ...\n $ Gender   : chr [1:139] \"Male\" \"Female\" \"Female\" \"Female\" ...\n\n\nQ4. Check your codebook to see what variables need to be reverse-coded.\nWe see that items FIN_1_3 and FIN_2_7 need to be reverse coded.\n\nreverse_scores = c(\"FIN_1_3\", \"FIN_2_7\")\ndf [ , reverse_scores] = 8 - df [ , reverse_scores]\n\nQ5. Check your codebook to see which variable needs to be computed.\nCodebook shows that items starting with FIN need to be averaged to give us a new variable, SFSS_A.\n\ndf$SFSS_A <- rowMeans(df[,c(\"FIN_1_1\", \"FIN_1_2\", \"FIN_1_3\", \"FIN_1_4\", \"FIN_1_5\", \"FIN_1_6\", \"FIN_1_7\", \"FIN_1_8\", \"FIN_2_1\", \"FIN_2_2\", \"FIN_2_3\", \"FIN_2_4\", \"FIN_2_5\", \"FIN_2_6\", \"FIN_2_7\")], na.rm = TRUE)\n\n\n\n\nLet’s have a refresher on what means and standard deviations are.\nQ1. Select the variables ‘COV_1’, ‘COV_2’ and ‘COV_3’ and look at the mean scores, standard deviations, and histograms for each question.\n\nOverall, what were survey respondents most worried about at the beginning of lockdown?\nFor which variable does the amount of worry vary the least between respondents?\nWhat is the shape of each distribution? Are any ceiling or floor effects present?\n\n\npsych::describe(df [ ,c(\"COV_1\", \"COV_2\", \"COV_3\")], na.rm = TRUE)\n\n      vars   n mean   sd median trimmed  mad min max range  skew kurtosis   se\nCOV_1    1 139 2.63 1.02      3    2.63 1.48   1   5     4 -0.02    -0.72 0.09\nCOV_2    2 139 2.36 1.17      2    2.28 1.48   1   5     4  0.46    -0.88 0.10\nCOV_3    3 139 3.57 1.17      4    3.65 1.48   1   5     4 -0.45    -0.74 0.10\n\n\n\nlibrary(ggplot2)\nggplot(df, aes(COV_1)) + geom_histogram(binwidth = 1)\nggplot(df, aes(COV_2)) + geom_histogram(binwidth = 1)\nggplot(df, aes(COV_3)) + geom_histogram(binwidth = 1)\n\n\n\n\n\n\nCOV _ 1\n\n\n\n\n\n\n\nCOV _ 2\n\n\n\n\n\n\n\nCOV _ 3\n\n\n\n\n\n\nNow we will look at another variable more deeply and figure out if their distribution is normal. This is important as we go into inferential statistics, we are required to check all variables for the assumption of normality (amongst other assumption tests) before we run our analyses.\nQ2. Get a histogram and a boxplot for COV_4\n\nggplot(df, aes(COV_4)) + geom_histogram(binwidth = 1)\n\nWarning: Removed 2 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\n\nboxplot(df$COV_4)\n\n\n\n\nQ3. Calculate Mean, Standard deviation, Skewness, and Shapiro-Wilk.\n\npsych::describe(df$COV_4)\n\n   vars   n mean   sd median trimmed  mad min max range  skew kurtosis   se\nX1    1 137 6.58 2.19    7.1    6.68 2.22 1.8  10   8.2 -0.41    -0.84 0.19\n\nshapiro.test(df$COV_4)\n\n\n    Shapiro-Wilk normality test\n\ndata:  df$COV_4\nW = 0.94971, p-value = 6.908e-05\n\n\nQ4. What do these statistics, alongside the graphs, tell us about the distribution of responses to this question?\n\nWhere is it centred?\nIs it normal?\nIs it skewed?\n\n\n\n\nWe now want to test two directionality hypotheses to have a basic understanding of how our variables relate to each other. This is a step called exploratory data analysis.\nFirstly, we want to know the directionality of the relationship between how well students coped during lockdown ‘COV_4’ and their impression of how the pandemic has impacted their academic performance ‘COV_10’.\nQ1. Calculate the correlation between COV_4 and COV_10. using ‘Kendall’s tau-b’. NOTE: We use this statistic because COV_10 is technically ordinal, rather than scale. You use the same guidelines as Pearson’s r to assess strength i.e. 0.1, 0.3, and 0.5 for weak, moderate, and strong, respectively.\n\ncor.test(df$COV_4, df$COV_10, method = c (\"kendall\"))\n\n\n    Kendall's rank correlation tau\n\ndata:  df$COV_4 and df$COV_10\nz = -4.4807, p-value = 7.441e-06\nalternative hypothesis: true tau is not equal to 0\nsample estimates:\n       tau \n-0.3185941 \n\n\nQ2. What do the correlation coefficient tell us about the relationship between how well someone thought they coped overall during lockdown (COV_4) and how they feel the pandemic affected their academic performance (COV_10). Make note of: a. The direction of the correlation. HINT: Refer to the codebook for how the response scales are for both variables. b. The strength of the correlation. c. Whether the correlation is statistically significant.\nQ3. Do you think having a worse experience of lockdown overall tended to result in poorer academic performance, or did poorer academic performance result in a worse experience of lockdown overall? What do the correlational data tell us about which is more likely?\nQ4. We want to know the directionality of the relationship between how well students coped during lockdown ‘COV_4’ and their financial stress. Calculate correlation between COV_4 and the financial stress composite variable that you created, using ‘Pearson’ correlation.\n\ncor.test(df$COV_4, df$SFSS_A, method = c (\"pearson\"))\n\n\n    Pearson's product-moment correlation\n\ndata:  df$COV_4 and df$SFSS_A\nt = -0.56441, df = 135, p-value = 0.5734\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.2144895  0.1201740\nsample estimates:\n       cor \n-0.0485194 \n\n\nQ5.. What does the correlation coefficient tell us about the relationship between how well someone thought they coped overall during lockdown (COV_4) and their financial stress?\nThe End"
  },
  {
    "objectID": "labs/what-is-quarto/index.html",
    "href": "labs/what-is-quarto/index.html",
    "title": "Lab 4: Exploratory and Confirmatory Factor Analyses",
    "section": "",
    "text": "In this lab we will have a look at a scale used in Prof Andy Field’s textbook (Discovering Statistics Using IBM SPSS Statistics) to conduct EFA and CFA analyses. The purpose of the factor analysis exercise is to get a better understanding of the psychological construct (SPSS Anxiety) as measured by the SAQ (i.e., construct validity information). Knowing the construct validity of a scale allows us to make an informed decision on how the scale can be used. One of the most common usages is to create valid composite scores. We randomly divided up the original dataset of responses on the SAQ into an EFA sample and a CFA sample (Please note that Usman used the same dataset for EFA in the class too, but he used the data from all subjects). Dividing up is a common methodological strategy in scale development to obtain two independent samples. This lab is important because it provides you with an experiential learning opportunity to conduct exploratory and confirmatory factor analyses. This is an important skill that will be assessed in your Lab Report (the next assignment)."
  },
  {
    "objectID": "labs/what-is-quarto/index.html#task-1-exploratory-factor-analysis-deciding-on-a-factor-structure",
    "href": "labs/what-is-quarto/index.html#task-1-exploratory-factor-analysis-deciding-on-a-factor-structure",
    "title": "Lab 4: Exploratory and Confirmatory Factor Analyses",
    "section": "Task 1: Exploratory Factor Analysis – Deciding on a factor structure",
    "text": "Task 1: Exploratory Factor Analysis – Deciding on a factor structure\nQ1. Import the dataset into R. Go through to ensure that the data variable information is correct for all the variables (i.e., measure type, data type, missing values).\n\nlibrary(readxl)\ndf <- read_xlsx(\"Lab 4 Dataset.xlsx\")\ndf\n\n# A tibble: 2,571 × 24\n   factor_anal…¹ Quest…² Quest…³ Quest…⁴ Quest…⁵ Quest…⁶ Quest…⁷ Quest…⁸ Quest…⁹\n           <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1             1       2       1       4       2       2       2       3       1\n 2             1       1       1       4       3       2       2       2       2\n 3             1       2       3       2       2       4       1       2       2\n 4             1       3       1       1       4       3       3       4       2\n 5             1       2       1       3       2       2       3       3       2\n 6             1       2       1       3       2       4       4       4       2\n 7             1       2       3       3       2       2       2       2       2\n 8             1       2       2       3       2       2       2       2       2\n 9             1       3       3       1       4       5       3       5       5\n10             1       2       4       4       3       2       1       2       2\n# … with 2,561 more rows, 15 more variables: Question_9 <dbl>,\n#   Question_10 <dbl>, Question_11 <dbl>, Question_12 <dbl>, Question_13 <dbl>,\n#   Question_14 <dbl>, Question_15 <dbl>, Question_16 <dbl>, Question_17 <dbl>,\n#   Question_18 <dbl>, Question_19 <dbl>, Question_20 <dbl>, Question_21 <dbl>,\n#   Question_22 <dbl>, Question_23 <dbl>, and abbreviated variable names\n#   ¹​factor_analysis_sample, ²​Question_1, ³​Question_2, ⁴​Question_3,\n#   ⁵​Question_4, ⁶​Question_5, ⁷​Question_6, ⁸​Question_7, ⁹​Question_8\n\n\nLet’s have a look at the structure.\n\nstr(df)\n\ntibble [2,571 × 24] (S3: tbl_df/tbl/data.frame)\n $ factor_analysis_sample: num [1:2571] 1 1 1 1 1 1 1 1 1 1 ...\n $ Question_1            : num [1:2571] 2 1 2 3 2 2 2 2 3 2 ...\n $ Question_2            : num [1:2571] 1 1 3 1 1 1 3 2 3 4 ...\n $ Question_3            : num [1:2571] 4 4 2 1 3 3 3 3 1 4 ...\n $ Question_4            : num [1:2571] 2 3 2 4 2 2 2 2 4 3 ...\n $ Question_5            : num [1:2571] 2 2 4 3 2 4 2 2 5 2 ...\n $ Question_6            : num [1:2571] 2 2 1 3 3 4 2 2 3 1 ...\n $ Question_7            : num [1:2571] 3 2 2 4 3 4 2 2 5 2 ...\n $ Question_8            : num [1:2571] 1 2 2 2 2 2 2 2 5 2 ...\n $ Question_9            : num [1:2571] 1 5 2 2 4 4 3 4 3 3 ...\n $ Question_10           : num [1:2571] 2 2 2 4 2 3 2 2 3 2 ...\n $ Question_11           : num [1:2571] 1 2 3 2 2 2 2 2 5 2 ...\n $ Question_12           : num [1:2571] 2 3 3 2 3 4 2 3 5 3 ...\n $ Question_13           : num [1:2571] 2 1 2 2 3 3 2 2 5 2 ...\n $ Question_14           : num [1:2571] 2 3 4 3 2 3 2 2 5 1 ...\n $ Question_15           : num [1:2571] 2 4 2 3 2 5 2 3 5 2 ...\n $ Question_16           : num [1:2571] 3 3 3 3 2 2 2 2 5 3 ...\n $ Question_17           : num [1:2571] 1 2 2 2 2 3 2 2 5 2 ...\n $ Question_18           : num [1:2571] 2 2 3 4 3 5 2 2 5 2 ...\n $ Question_19           : num [1:2571] 3 3 1 2 3 1 3 4 2 3 ...\n $ Question_20           : num [1:2571] 2 4 4 4 4 5 2 3 5 3 ...\n $ Question_21           : num [1:2571] 2 4 3 4 2 3 2 2 5 2 ...\n $ Question_22           : num [1:2571] 2 4 2 4 4 1 4 4 3 4 ...\n $ Question_23           : num [1:2571] 5 2 2 3 4 4 4 4 3 4 ...\n\n\nQ2. As a practice, let’s rename the nominal (factor analysis sample) variable levels to corresponding names where EFA replaces 1 and CFA replaces 2.\n\ncol <- 1\ndf[col] <- lapply(df[col], as.character)\nstr(df)\n\ntibble [2,571 × 24] (S3: tbl_df/tbl/data.frame)\n $ factor_analysis_sample: chr [1:2571] \"1\" \"1\" \"1\" \"1\" ...\n $ Question_1            : num [1:2571] 2 1 2 3 2 2 2 2 3 2 ...\n $ Question_2            : num [1:2571] 1 1 3 1 1 1 3 2 3 4 ...\n $ Question_3            : num [1:2571] 4 4 2 1 3 3 3 3 1 4 ...\n $ Question_4            : num [1:2571] 2 3 2 4 2 2 2 2 4 3 ...\n $ Question_5            : num [1:2571] 2 2 4 3 2 4 2 2 5 2 ...\n $ Question_6            : num [1:2571] 2 2 1 3 3 4 2 2 3 1 ...\n $ Question_7            : num [1:2571] 3 2 2 4 3 4 2 2 5 2 ...\n $ Question_8            : num [1:2571] 1 2 2 2 2 2 2 2 5 2 ...\n $ Question_9            : num [1:2571] 1 5 2 2 4 4 3 4 3 3 ...\n $ Question_10           : num [1:2571] 2 2 2 4 2 3 2 2 3 2 ...\n $ Question_11           : num [1:2571] 1 2 3 2 2 2 2 2 5 2 ...\n $ Question_12           : num [1:2571] 2 3 3 2 3 4 2 3 5 3 ...\n $ Question_13           : num [1:2571] 2 1 2 2 3 3 2 2 5 2 ...\n $ Question_14           : num [1:2571] 2 3 4 3 2 3 2 2 5 1 ...\n $ Question_15           : num [1:2571] 2 4 2 3 2 5 2 3 5 2 ...\n $ Question_16           : num [1:2571] 3 3 3 3 2 2 2 2 5 3 ...\n $ Question_17           : num [1:2571] 1 2 2 2 2 3 2 2 5 2 ...\n $ Question_18           : num [1:2571] 2 2 3 4 3 5 2 2 5 2 ...\n $ Question_19           : num [1:2571] 3 3 1 2 3 1 3 4 2 3 ...\n $ Question_20           : num [1:2571] 2 4 4 4 4 5 2 3 5 3 ...\n $ Question_21           : num [1:2571] 2 4 3 4 2 3 2 2 5 2 ...\n $ Question_22           : num [1:2571] 2 4 2 4 4 1 4 4 3 4 ...\n $ Question_23           : num [1:2571] 5 2 2 3 4 4 4 4 3 4 ...\n\n\nQ3. Here is a neat trick; R (and other data analysis software) use the filter function that enables you to work with a subset of a large dataset. This is handy for keeping analyses tidy. For this exercise, we are going to use a filter to separate out our EFA and CFA samples. We designate 1 to the EFA subsample and 2 to the CFA sample.\nRenaming EFA and CFA samples:\n\ndf$FAS = ifelse(df$factor_analysis_sample < 2, \"EFA\", \"CFA\")\nstr(df)\n\ntibble [2,571 × 25] (S3: tbl_df/tbl/data.frame)\n $ factor_analysis_sample: chr [1:2571] \"1\" \"1\" \"1\" \"1\" ...\n $ Question_1            : num [1:2571] 2 1 2 3 2 2 2 2 3 2 ...\n $ Question_2            : num [1:2571] 1 1 3 1 1 1 3 2 3 4 ...\n $ Question_3            : num [1:2571] 4 4 2 1 3 3 3 3 1 4 ...\n $ Question_4            : num [1:2571] 2 3 2 4 2 2 2 2 4 3 ...\n $ Question_5            : num [1:2571] 2 2 4 3 2 4 2 2 5 2 ...\n $ Question_6            : num [1:2571] 2 2 1 3 3 4 2 2 3 1 ...\n $ Question_7            : num [1:2571] 3 2 2 4 3 4 2 2 5 2 ...\n $ Question_8            : num [1:2571] 1 2 2 2 2 2 2 2 5 2 ...\n $ Question_9            : num [1:2571] 1 5 2 2 4 4 3 4 3 3 ...\n $ Question_10           : num [1:2571] 2 2 2 4 2 3 2 2 3 2 ...\n $ Question_11           : num [1:2571] 1 2 3 2 2 2 2 2 5 2 ...\n $ Question_12           : num [1:2571] 2 3 3 2 3 4 2 3 5 3 ...\n $ Question_13           : num [1:2571] 2 1 2 2 3 3 2 2 5 2 ...\n $ Question_14           : num [1:2571] 2 3 4 3 2 3 2 2 5 1 ...\n $ Question_15           : num [1:2571] 2 4 2 3 2 5 2 3 5 2 ...\n $ Question_16           : num [1:2571] 3 3 3 3 2 2 2 2 5 3 ...\n $ Question_17           : num [1:2571] 1 2 2 2 2 3 2 2 5 2 ...\n $ Question_18           : num [1:2571] 2 2 3 4 3 5 2 2 5 2 ...\n $ Question_19           : num [1:2571] 3 3 1 2 3 1 3 4 2 3 ...\n $ Question_20           : num [1:2571] 2 4 4 4 4 5 2 3 5 3 ...\n $ Question_21           : num [1:2571] 2 4 3 4 2 3 2 2 5 2 ...\n $ Question_22           : num [1:2571] 2 4 2 4 4 1 4 4 3 4 ...\n $ Question_23           : num [1:2571] 5 2 2 3 4 4 4 4 3 4 ...\n $ FAS                   : chr [1:2571] \"EFA\" \"EFA\" \"EFA\" \"EFA\" ...\n\n\nSelecting EFA sample only, so it can be used for further analysis.\n\nEFA <- dplyr::filter(df, FAS %in% c(\"EFA\"))\nstr(EFA)\n\ntibble [1,285 × 25] (S3: tbl_df/tbl/data.frame)\n $ factor_analysis_sample: chr [1:1285] \"1\" \"1\" \"1\" \"1\" ...\n $ Question_1            : num [1:1285] 2 1 2 3 2 2 2 2 3 2 ...\n $ Question_2            : num [1:1285] 1 1 3 1 1 1 3 2 3 4 ...\n $ Question_3            : num [1:1285] 4 4 2 1 3 3 3 3 1 4 ...\n $ Question_4            : num [1:1285] 2 3 2 4 2 2 2 2 4 3 ...\n $ Question_5            : num [1:1285] 2 2 4 3 2 4 2 2 5 2 ...\n $ Question_6            : num [1:1285] 2 2 1 3 3 4 2 2 3 1 ...\n $ Question_7            : num [1:1285] 3 2 2 4 3 4 2 2 5 2 ...\n $ Question_8            : num [1:1285] 1 2 2 2 2 2 2 2 5 2 ...\n $ Question_9            : num [1:1285] 1 5 2 2 4 4 3 4 3 3 ...\n $ Question_10           : num [1:1285] 2 2 2 4 2 3 2 2 3 2 ...\n $ Question_11           : num [1:1285] 1 2 3 2 2 2 2 2 5 2 ...\n $ Question_12           : num [1:1285] 2 3 3 2 3 4 2 3 5 3 ...\n $ Question_13           : num [1:1285] 2 1 2 2 3 3 2 2 5 2 ...\n $ Question_14           : num [1:1285] 2 3 4 3 2 3 2 2 5 1 ...\n $ Question_15           : num [1:1285] 2 4 2 3 2 5 2 3 5 2 ...\n $ Question_16           : num [1:1285] 3 3 3 3 2 2 2 2 5 3 ...\n $ Question_17           : num [1:1285] 1 2 2 2 2 3 2 2 5 2 ...\n $ Question_18           : num [1:1285] 2 2 3 4 3 5 2 2 5 2 ...\n $ Question_19           : num [1:1285] 3 3 1 2 3 1 3 4 2 3 ...\n $ Question_20           : num [1:1285] 2 4 4 4 4 5 2 3 5 3 ...\n $ Question_21           : num [1:1285] 2 4 3 4 2 3 2 2 5 2 ...\n $ Question_22           : num [1:1285] 2 4 2 4 4 1 4 4 3 4 ...\n $ Question_23           : num [1:1285] 5 2 2 3 4 4 4 4 3 4 ...\n $ FAS                   : chr [1:1285] \"EFA\" \"EFA\" \"EFA\" \"EFA\" ...\n\n\nFirst, let’s select only numeric columns - pertaining to questionnaire items.\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nEFA_items <- EFA %>%\n  select(Question_1, Question_2, Question_3, Question_4, Question_5, Question_6, Question_7, Question_8, Question_9, Question_10, Question_11, Question_12, Question_13, Question_14, Question_15, Question_16, Question_17, Question_18, Question_19, Question_20, Question_21, Question_22, Question_23)\nstr(EFA_items)\n\ntibble [1,285 × 23] (S3: tbl_df/tbl/data.frame)\n $ Question_1 : num [1:1285] 2 1 2 3 2 2 2 2 3 2 ...\n $ Question_2 : num [1:1285] 1 1 3 1 1 1 3 2 3 4 ...\n $ Question_3 : num [1:1285] 4 4 2 1 3 3 3 3 1 4 ...\n $ Question_4 : num [1:1285] 2 3 2 4 2 2 2 2 4 3 ...\n $ Question_5 : num [1:1285] 2 2 4 3 2 4 2 2 5 2 ...\n $ Question_6 : num [1:1285] 2 2 1 3 3 4 2 2 3 1 ...\n $ Question_7 : num [1:1285] 3 2 2 4 3 4 2 2 5 2 ...\n $ Question_8 : num [1:1285] 1 2 2 2 2 2 2 2 5 2 ...\n $ Question_9 : num [1:1285] 1 5 2 2 4 4 3 4 3 3 ...\n $ Question_10: num [1:1285] 2 2 2 4 2 3 2 2 3 2 ...\n $ Question_11: num [1:1285] 1 2 3 2 2 2 2 2 5 2 ...\n $ Question_12: num [1:1285] 2 3 3 2 3 4 2 3 5 3 ...\n $ Question_13: num [1:1285] 2 1 2 2 3 3 2 2 5 2 ...\n $ Question_14: num [1:1285] 2 3 4 3 2 3 2 2 5 1 ...\n $ Question_15: num [1:1285] 2 4 2 3 2 5 2 3 5 2 ...\n $ Question_16: num [1:1285] 3 3 3 3 2 2 2 2 5 3 ...\n $ Question_17: num [1:1285] 1 2 2 2 2 3 2 2 5 2 ...\n $ Question_18: num [1:1285] 2 2 3 4 3 5 2 2 5 2 ...\n $ Question_19: num [1:1285] 3 3 1 2 3 1 3 4 2 3 ...\n $ Question_20: num [1:1285] 2 4 4 4 4 5 2 3 5 3 ...\n $ Question_21: num [1:1285] 2 4 3 4 2 3 2 2 5 2 ...\n $ Question_22: num [1:1285] 2 4 2 4 4 1 4 4 3 4 ...\n $ Question_23: num [1:1285] 5 2 2 3 4 4 4 4 3 4 ...\n\n\nQ4. Set extraction method to ‘Principal axis’ (allowing for measurement error with the new scale being developed) and rotation method to ‘Promax’ (allowing for factors to correlate because most psychological constructs do correlate to some extent). Number of factors should be based on eigenvalues (Eigenvalues greater than 1). Hide loadings below 0.4; and show Factor summary.\nQ5. Check both the options under ‘assumption checks’ (Bartlett’s test of sphericity and KMO measure of sampling adequacy). Do our data satisfy both assumptions for EFA?\nQ6. With the criteria of eigenvalues greater than 1, how many factors were extracted?\nQ7. Since a 1-factor model implies that SAQ is a 23-item scale, to explore how to make the scale more parsimonious we shall rerun the factor analysis with a more liberal eigenvalue. Let’s try eigenvalues greater than 0, how many factors were extracted now?\nCorrelate all items and round it up two dp.\n\nEFAMatrix <- cor(EFA_items)\ncored<-round (EFAMatrix, 2)\ncorrplot::corrplot(cored)\n\n\n\n\nChecking EFA assumptions\n\npsych::cortest.bartlett(EFAMatrix, n = 1285)\n\n$chisq\n[1] 9873.737\n\n$p.value\n[1] 0\n\n$df\n[1] 253\n\npsych::KMO(EFA_items)\n\nKaiser-Meyer-Olkin factor adequacy\nCall: psych::KMO(r = EFA_items)\nOverall MSA =  0.93\nMSA for each item = \n Question_1  Question_2  Question_3  Question_4  Question_5  Question_6 \n       0.91        0.87        0.95        0.95        0.96        0.90 \n Question_7  Question_8  Question_9 Question_10 Question_11 Question_12 \n       0.94        0.88        0.82        0.94        0.90        0.95 \nQuestion_13 Question_14 Question_15 Question_16 Question_17 Question_18 \n       0.94        0.96        0.93        0.94        0.93        0.95 \nQuestion_19 Question_20 Question_21 Question_22 Question_23 \n       0.93        0.85        0.92        0.85        0.72 \n\ndet(cor(EFAMatrix))\n\n[1] -3.268877e-30\n\n\nExtracting EFA factors.\nlength(EFA_items) tells us the number of items\n\nlen<-length(EFA_items)\nlen\n\n[1] 23\n\n\nWe are running principal axis factoring, with arbitrary number of 10 factors and no rotation.\n\npcModelnr<-psych::fa(EFA_items, nfactors = 10, fm = 'pa', rotate = \"none\")\npcModelnr\n\nFactor Analysis using method =  pa\nCall: psych::fa(r = EFA_items, nfactors = 10, rotate = \"none\", fm = \"pa\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n              PA1   PA2   PA3   PA4   PA5   PA6   PA7   PA8   PA9  PA10   h2\nQuestion_1   0.54  0.12 -0.25  0.21 -0.29 -0.21 -0.08  0.09  0.03  0.10 0.57\nQuestion_2  -0.29  0.39  0.09  0.15  0.00  0.02 -0.07 -0.09  0.05  0.08 0.29\nQuestion_3  -0.61  0.26  0.14  0.06  0.03  0.04  0.05 -0.02  0.12  0.01 0.48\nQuestion_4   0.62  0.09 -0.15  0.13 -0.13  0.02 -0.06  0.05 -0.06 -0.03 0.46\nQuestion_5   0.53  0.05 -0.11  0.10 -0.19 -0.04 -0.17  0.03  0.02 -0.05 0.37\nQuestion_6   0.57  0.05  0.52 -0.02  0.08  0.03 -0.21  0.14  0.03  0.11 0.68\nQuestion_7   0.69  0.03  0.22  0.09  0.14  0.05 -0.17  0.01 -0.17 -0.16 0.64\nQuestion_8   0.58  0.41 -0.20 -0.32  0.09 -0.01  0.00 -0.02 -0.06  0.00 0.65\nQuestion_9  -0.28  0.51  0.05  0.24  0.10 -0.05 -0.01 -0.12 -0.10  0.08 0.44\nQuestion_10  0.42  0.02  0.22 -0.02 -0.15  0.08 -0.04 -0.01  0.21 -0.01 0.30\nQuestion_11  0.67  0.29 -0.14 -0.39  0.09  0.01  0.03  0.14  0.09  0.00 0.75\nQuestion_12  0.64 -0.10 -0.01  0.20  0.01 -0.08  0.19  0.01 -0.03 -0.10 0.52\nQuestion_13  0.66  0.07  0.24  0.05  0.09 -0.14  0.24  0.07  0.08 -0.05 0.59\nQuestion_14  0.63 -0.02  0.14  0.08  0.03 -0.04  0.13 -0.13 -0.10  0.18 0.50\nQuestion_15  0.56 -0.06  0.14 -0.07 -0.19  0.44  0.10 -0.07 -0.06  0.03 0.58\nQuestion_16  0.67 -0.01 -0.11  0.11 -0.27  0.10  0.04 -0.10  0.06  0.01 0.57\nQuestion_17  0.63  0.34 -0.09 -0.21  0.02 -0.03  0.02 -0.13 -0.03 -0.02 0.59\nQuestion_18  0.67  0.00  0.23  0.14  0.06 -0.15  0.08  0.01  0.04  0.00 0.55\nQuestion_19 -0.37  0.29  0.06  0.05 -0.08  0.05  0.02 -0.02  0.16 -0.09 0.27\nQuestion_20  0.38 -0.20 -0.32  0.12  0.29  0.14 -0.01  0.11  0.09  0.18 0.47\nQuestion_21  0.65 -0.09 -0.22  0.25  0.32  0.08 -0.10 -0.18  0.13 -0.09 0.72\nQuestion_22 -0.27  0.34 -0.08  0.23  0.04  0.15  0.05  0.09  0.00 -0.07 0.29\nQuestion_23 -0.12  0.20 -0.08  0.23  0.01  0.15  0.10  0.27 -0.10 -0.01 0.23\n              u2 com\nQuestion_1  0.43 3.1\nQuestion_2  0.71 2.7\nQuestion_3  0.52 1.6\nQuestion_4  0.54 1.4\nQuestion_5  0.63 1.7\nQuestion_6  0.32 2.6\nQuestion_7  0.36 1.7\nQuestion_8  0.35 2.8\nQuestion_9  0.56 2.5\nQuestion_10 0.70 2.5\nQuestion_11 0.25 2.3\nQuestion_12 0.48 1.5\nQuestion_13 0.41 1.8\nQuestion_14 0.50 1.6\nQuestion_15 0.42 2.5\nQuestion_16 0.43 1.6\nQuestion_17 0.41 2.0\nQuestion_18 0.45 1.5\nQuestion_19 0.73 2.7\nQuestion_20 0.53 5.0\nQuestion_21 0.28 2.6\nQuestion_22 0.71 3.7\nQuestion_23 0.77 4.9\n\n                       PA1  PA2  PA3  PA4  PA5  PA6  PA7  PA8  PA9 PA10\nSS loadings           6.88 1.18 0.89 0.73 0.54 0.39 0.27 0.25 0.20 0.16\nProportion Var        0.30 0.05 0.04 0.03 0.02 0.02 0.01 0.01 0.01 0.01\nCumulative Var        0.30 0.35 0.39 0.42 0.44 0.46 0.47 0.48 0.49 0.50\nProportion Explained  0.60 0.10 0.08 0.06 0.05 0.03 0.02 0.02 0.02 0.01\nCumulative Proportion 0.60 0.70 0.78 0.84 0.89 0.92 0.95 0.97 0.99 1.00\n\nMean item complexity =  2.5\nTest of the hypothesis that 10 factors are sufficient.\n\nThe degrees of freedom for the null model are  253  and the objective function was  7.74 with Chi Square of  9873.74\nThe degrees of freedom for the model are 68  and the objective function was  0.09 \n\nThe root mean square of the residuals (RMSR) is  0.01 \nThe df corrected root mean square of the residuals is  0.02 \n\nThe harmonic number of observations is  1285 with the empirical chi square  63.4  with prob <  0.64 \nThe total number of observations was  1285  with Likelihood Chi Square =  111.33  with prob <  0.00072 \n\nTucker Lewis Index of factoring reliability =  0.983\nRMSEA index =  0.022  and the 90 % confidence intervals are  0.014 0.03\nBIC =  -375.45\nFit based upon off diagonal values = 1\nMeasures of factor score adequacy             \n                                                   PA1  PA2  PA3  PA4  PA5\nCorrelation of (regression) scores with factors   0.97 0.83 0.83 0.80 0.75\nMultiple R square of scores with factors          0.94 0.70 0.69 0.64 0.56\nMinimum correlation of possible factor scores     0.88 0.39 0.38 0.28 0.12\n                                                    PA6   PA7   PA8   PA9  PA10\nCorrelation of (regression) scores with factors    0.68  0.62  0.59  0.55  0.51\nMultiple R square of scores with factors           0.46  0.39  0.35  0.30  0.26\nMinimum correlation of possible factor scores     -0.08 -0.23 -0.30 -0.40 -0.49\n\n\nWe change rotation to oblimin.\n\npcModelnrob<-psych::fa(EFA_items, nfactors = 10, fm = 'pa', rotate = \"oblimin\")\n\nLoading required namespace: GPArotation\n\n\nWarning in GPFoblq(L, Tmat = Tmat, normalize = normalize, eps = eps, maxit =\nmaxit, : convergence not obtained in GPFoblq. 1000 iterations used.\n\npcModelnrob\n\nFactor Analysis using method =  pa\nCall: psych::fa(r = EFA_items, nfactors = 10, rotate = \"oblimin\", fm = \"pa\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n              PA4   PA7   PA1   PA5   PA3   PA2   PA6   PA8   PA9  PA10   h2\nQuestion_1   0.04  0.75  0.04 -0.02  0.01  0.02 -0.06  0.01 -0.03 -0.03 0.57\nQuestion_2  -0.02  0.03 -0.11 -0.01  0.08  0.49 -0.01 -0.01 -0.07  0.08 0.29\nQuestion_3  -0.10 -0.20 -0.02 -0.09 -0.04  0.35 -0.06  0.05 -0.14  0.15 0.48\nQuestion_4   0.12  0.40  0.02  0.09  0.04 -0.02  0.13  0.08  0.12 -0.02 0.46\nQuestion_5   0.06  0.47 -0.07  0.07  0.12 -0.08  0.04 -0.01  0.10  0.09 0.37\nQuestion_6   0.01  0.02  0.05 -0.01  0.76  0.02  0.05 -0.02  0.00 -0.01 0.68\nQuestion_7   0.09  0.02  0.09  0.15  0.38 -0.05  0.07  0.03  0.35 -0.02 0.64\nQuestion_8   0.81  0.03 -0.04  0.00 -0.04  0.07  0.01 -0.01  0.05 -0.05 0.65\nQuestion_9   0.03  0.00 -0.01 -0.01  0.01  0.63 -0.10  0.04  0.05 -0.06 0.44\nQuestion_10  0.00  0.13  0.09  0.03  0.24 -0.03  0.20 -0.10 -0.11  0.21 0.30\nQuestion_11  0.81  0.00  0.05  0.02  0.07 -0.12 -0.01  0.05 -0.08  0.05 0.75\nQuestion_12 -0.04  0.14  0.49  0.14 -0.10 -0.11  0.09  0.04  0.12 -0.04 0.52\nQuestion_13  0.14 -0.02  0.64  0.00  0.10 -0.02  0.00  0.01 -0.02  0.02 0.59\nQuestion_14  0.02  0.07  0.31  0.06  0.11  0.09  0.22 -0.15 -0.02 -0.25 0.50\nQuestion_15  0.05 -0.05 -0.01 -0.01  0.07 -0.05  0.72  0.03  0.01 -0.01 0.58\nQuestion_16  0.02  0.39  0.09  0.10 -0.08 -0.03  0.37 -0.08 -0.01  0.05 0.57\nQuestion_17  0.61  0.06  0.07  0.03 -0.03  0.11  0.09 -0.12  0.07 -0.01 0.59\nQuestion_18 -0.01  0.11  0.47  0.09  0.22  0.00 -0.01 -0.06  0.03 -0.03 0.55\nQuestion_19  0.00 -0.05 -0.02 -0.08 -0.08  0.27 -0.01  0.04 -0.08  0.26 0.27\nQuestion_20  0.03  0.02 -0.06  0.58  0.01 -0.10  0.01  0.15 -0.18 -0.17 0.47\nQuestion_21  0.02  0.01  0.06  0.79  0.00  0.04  0.01 -0.07  0.08  0.05 0.72\nQuestion_22 -0.01 -0.01 -0.01  0.05 -0.10  0.36  0.04  0.29  0.02  0.11 0.29\nQuestion_23 -0.06  0.08  0.06 -0.07 -0.03  0.18  0.07  0.43  0.03 -0.02 0.23\n              u2 com\nQuestion_1  0.43 1.0\nQuestion_2  0.71 1.3\nQuestion_3  0.52 3.0\nQuestion_4  0.54 1.9\nQuestion_5  0.63 1.5\nQuestion_6  0.32 1.0\nQuestion_7  0.36 2.7\nQuestion_8  0.35 1.0\nQuestion_9  0.56 1.1\nQuestion_10 0.70 4.8\nQuestion_11 0.25 1.1\nQuestion_12 0.48 1.8\nQuestion_13 0.41 1.2\nQuestion_14 0.50 4.1\nQuestion_15 0.42 1.1\nQuestion_16 0.43 2.5\nQuestion_17 0.41 1.3\nQuestion_18 0.45 1.7\nQuestion_19 0.73 2.7\nQuestion_20 0.53 1.7\nQuestion_21 0.28 1.1\nQuestion_22 0.71 2.4\nQuestion_23 0.77 1.6\n\n                       PA4  PA7  PA1  PA5  PA3  PA2  PA6  PA8  PA9 PA10\nSS loadings           2.06 1.64 1.56 1.41 1.25 1.24 1.18 0.44 0.38 0.34\nProportion Var        0.09 0.07 0.07 0.06 0.05 0.05 0.05 0.02 0.02 0.01\nCumulative Var        0.09 0.16 0.23 0.29 0.34 0.40 0.45 0.47 0.49 0.50\nProportion Explained  0.18 0.14 0.14 0.12 0.11 0.11 0.10 0.04 0.03 0.03\nCumulative Proportion 0.18 0.32 0.46 0.58 0.69 0.80 0.90 0.94 0.97 1.00\n\n With factor correlations of \n       PA4   PA7   PA1   PA5   PA3   PA2   PA6   PA8   PA9  PA10\nPA4   1.00  0.49  0.46  0.42  0.38 -0.13  0.42 -0.12  0.20 -0.11\nPA7   0.49  1.00  0.50  0.48  0.23 -0.17  0.43 -0.12  0.25 -0.14\nPA1   0.46  0.50  1.00  0.47  0.56 -0.19  0.46 -0.22  0.27 -0.15\nPA5   0.42  0.48  0.47  1.00  0.27 -0.30  0.38 -0.03  0.28 -0.28\nPA3   0.38  0.23  0.56  0.27  1.00 -0.17  0.43 -0.19  0.21 -0.04\nPA2  -0.13 -0.17 -0.19 -0.30 -0.17  1.00 -0.28  0.13  0.05  0.19\nPA6   0.42  0.43  0.46  0.38  0.43 -0.28  1.00 -0.17  0.22 -0.07\nPA8  -0.12 -0.12 -0.22 -0.03 -0.19  0.13 -0.17  1.00 -0.16  0.01\nPA9   0.20  0.25  0.27  0.28  0.21  0.05  0.22 -0.16  1.00 -0.05\nPA10 -0.11 -0.14 -0.15 -0.28 -0.04  0.19 -0.07  0.01 -0.05  1.00\n\nMean item complexity =  1.9\nTest of the hypothesis that 10 factors are sufficient.\n\nThe degrees of freedom for the null model are  253  and the objective function was  7.74 with Chi Square of  9873.74\nThe degrees of freedom for the model are 68  and the objective function was  0.09 \n\nThe root mean square of the residuals (RMSR) is  0.01 \nThe df corrected root mean square of the residuals is  0.02 \n\nThe harmonic number of observations is  1285 with the empirical chi square  63.4  with prob <  0.64 \nThe total number of observations was  1285  with Likelihood Chi Square =  111.33  with prob <  0.00072 \n\nTucker Lewis Index of factoring reliability =  0.983\nRMSEA index =  0.022  and the 90 % confidence intervals are  0.014 0.03\nBIC =  -375.45\nFit based upon off diagonal values = 1\nMeasures of factor score adequacy             \n                                                   PA4  PA7  PA1  PA5  PA3  PA2\nCorrelation of (regression) scores with factors   0.93 0.87 0.88 0.89 0.87 0.81\nMultiple R square of scores with factors          0.86 0.76 0.77 0.78 0.76 0.66\nMinimum correlation of possible factor scores     0.72 0.52 0.54 0.57 0.51 0.32\n                                                   PA6   PA8   PA9  PA10\nCorrelation of (regression) scores with factors   0.84  0.63  0.66  0.60\nMultiple R square of scores with factors          0.70  0.40  0.43  0.35\nMinimum correlation of possible factor scores     0.41 -0.21 -0.14 -0.29\n\n\nQ8. Since a X-factor model is not parsimonious either, let’s look at the scree plot. According to the scree plot, after what number of factors does it seems like minimal additional variance is explained?\n\nplot (pcModelnrob$values, type = \"b\")\n\n\n\n\nQ9. Fix the EFA to that number and look to see if the factor loadings of the items make intuitive sense by looking at which scale items are included in each factor loading. You will need to refer to the scale items for reflection (see The SPSS Anxiety Questionnaire (SAQ) png file on Learn). Reduce the number of factors by 1 and explore that factor structure the same way, and repeat by reducing that number of factors by 1 again. Take your time with this and use another sheet within your codebook to help you understand various factor structures. You can also talk to a friend from class/your teammate and get their opinions on this (a common practice amongst psychology researchers while doing a factor analysis). After exploring a couple of options, which factor model seems to make the most sense?\nScree plot shows up to 4 factors, so we restrict the number of factors to four.\n\npcModel4f<-psych::fa(EFA_items, nfactors = 4, fm = 'pa', rotate = \"oblimin\")\npcModel4f\n\nFactor Analysis using method =  pa\nCall: psych::fa(r = EFA_items, nfactors = 4, rotate = \"oblimin\", fm = \"pa\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n              PA1   PA3   PA4   PA2    h2   u2 com\nQuestion_1   0.57 -0.02  0.13  0.06 0.383 0.62 1.1\nQuestion_2  -0.06  0.06  0.00  0.52 0.277 0.72 1.1\nQuestion_3  -0.32 -0.04 -0.11  0.41 0.474 0.53 2.1\nQuestion_4   0.54  0.07  0.16  0.03 0.450 0.55 1.2\nQuestion_5   0.42  0.09  0.12 -0.02 0.308 0.69 1.3\nQuestion_6  -0.14  0.79  0.03  0.00 0.548 0.45 1.1\nQuestion_7   0.20  0.52  0.08 -0.03 0.515 0.49 1.4\nQuestion_8   0.01 -0.07  0.85  0.05 0.667 0.33 1.0\nQuestion_9   0.02  0.03  0.02  0.63 0.366 0.63 1.0\nQuestion_10  0.02  0.41  0.06 -0.03 0.222 0.78 1.1\nQuestion_11 -0.04  0.05  0.78 -0.11 0.673 0.33 1.1\nQuestion_12  0.49  0.27 -0.06 -0.10 0.468 0.53 1.7\nQuestion_13  0.13  0.54  0.11 -0.01 0.481 0.52 1.2\nQuestion_14  0.23  0.43  0.05 -0.07 0.413 0.59 1.6\nQuestion_15  0.13  0.31  0.12 -0.14 0.300 0.70 2.1\nQuestion_16  0.49  0.14  0.11 -0.08 0.468 0.53 1.3\nQuestion_17  0.09  0.12  0.64  0.06 0.567 0.43 1.1\nQuestion_18  0.24  0.58 -0.03 -0.01 0.526 0.47 1.3\nQuestion_19 -0.16 -0.05  0.01  0.36 0.224 0.78 1.4\nQuestion_20  0.42 -0.14  0.03 -0.22 0.235 0.77 1.8\nQuestion_21  0.54  0.09  0.05 -0.11 0.448 0.55 1.2\nQuestion_22  0.15 -0.14 -0.03  0.47 0.244 0.76 1.4\nQuestion_23  0.20 -0.08 -0.07  0.29 0.095 0.91 2.1\n\n                       PA1  PA3  PA4  PA2\nSS loadings           2.78 2.71 2.26 1.61\nProportion Var        0.12 0.12 0.10 0.07\nCumulative Var        0.12 0.24 0.34 0.41\nProportion Explained  0.30 0.29 0.24 0.17\nCumulative Proportion 0.30 0.59 0.83 1.00\n\n With factor correlations of \n      PA1   PA3   PA4   PA2\nPA1  1.00  0.53  0.55 -0.37\nPA3  0.53  1.00  0.52 -0.35\nPA4  0.55  0.52  1.00 -0.23\nPA2 -0.37 -0.35 -0.23  1.00\n\nMean item complexity =  1.4\nTest of the hypothesis that 4 factors are sufficient.\n\nThe degrees of freedom for the null model are  253  and the objective function was  7.74 with Chi Square of  9873.74\nThe degrees of freedom for the model are 167  and the objective function was  0.55 \n\nThe root mean square of the residuals (RMSR) is  0.03 \nThe df corrected root mean square of the residuals is  0.04 \n\nThe harmonic number of observations is  1285 with the empirical chi square  534.9  with prob <  4.2e-40 \nThe total number of observations was  1285  with Likelihood Chi Square =  698.46  with prob <  4.2e-66 \n\nTucker Lewis Index of factoring reliability =  0.916\nRMSEA index =  0.05  and the 90 % confidence intervals are  0.046 0.054\nBIC =  -497.01\nFit based upon off diagonal values = 0.99\nMeasures of factor score adequacy             \n                                                   PA1  PA3  PA4  PA2\nCorrelation of (regression) scores with factors   0.90 0.91 0.92 0.83\nMultiple R square of scores with factors          0.81 0.83 0.85 0.70\nMinimum correlation of possible factor scores     0.62 0.65 0.71 0.39\n\n\nQ10. How much total variance is accounted for by this final factor solution (e.g., total cumulative%)? What is the amount of variance accounted for by each factor?\nQ12. Make a note with yourself on the final items and which factor they belong to.\nQ13. Name each of the factors. What is your conclusion from the EFA?\n\npsych::fa.diagram(pcModel4f)"
  },
  {
    "objectID": "labs/what-is-quarto/index.html#task-2-confirmatory-factor-analysis",
    "href": "labs/what-is-quarto/index.html#task-2-confirmatory-factor-analysis",
    "title": "Lab 4: Exploratory and Confirmatory Factor Analyses",
    "section": "Task 2: Confirmatory Factor Analysis",
    "text": "Task 2: Confirmatory Factor Analysis\nQ1. Change your filter to select the CFA subsample. For this task, we want to examine whether the decision on how items should load on the SAQ based on EFA can be replicated on an independent sample. We are going to refer to the criteria for model fit mentioned in the CFA lecture.\n\nCFA <- dplyr::filter(df, FAS%in% c(\"CFA\"))\nstr(CFA)\n\ntibble [1,286 × 25] (S3: tbl_df/tbl/data.frame)\n $ factor_analysis_sample: chr [1:1286] \"2\" \"2\" \"2\" \"2\" ...\n $ Question_1            : num [1:1286] 4 3 2 3 3 2 2 3 2 2 ...\n $ Question_2            : num [1:1286] 1 1 2 2 1 1 1 1 1 1 ...\n $ Question_3            : num [1:1286] 1 1 3 3 1 2 3 2 3 4 ...\n $ Question_4            : num [1:1286] 4 5 4 4 4 3 3 3 2 2 ...\n $ Question_5            : num [1:1286] 4 4 2 4 2 3 4 4 3 3 ...\n $ Question_6            : num [1:1286] 2 2 3 4 2 3 2 1 1 2 ...\n $ Question_7            : num [1:1286] 2 2 5 4 5 3 5 2 1 1 ...\n $ Question_8            : num [1:1286] 2 2 3 3 1 2 2 3 1 1 ...\n $ Question_9            : num [1:1286] 1 1 4 2 1 1 4 5 1 5 ...\n $ Question_10           : num [1:1286] 3 2 1 4 3 3 3 5 2 3 ...\n $ Question_11           : num [1:1286] 2 2 1 3 3 2 3 2 1 1 ...\n $ Question_12           : num [1:1286] 4 5 5 4 5 3 3 4 3 3 ...\n $ Question_13           : num [1:1286] 2 1 4 4 3 3 4 2 1 1 ...\n $ Question_14           : num [1:1286] 2 3 4 4 4 3 3 2 1 2 ...\n $ Question_15           : num [1:1286] 2 1 2 4 2 3 5 3 3 4 ...\n $ Question_16           : num [1:1286] 4 5 2 4 2 3 2 3 2 3 ...\n $ Question_17           : num [1:1286] 3 2 4 4 3 3 2 3 2 2 ...\n $ Question_18           : num [1:1286] 4 3 5 4 3 2 4 3 1 1 ...\n $ Question_19           : num [1:1286] 2 1 5 2 3 2 2 4 4 5 ...\n $ Question_20           : num [1:1286] 5 5 3 4 2 3 4 5 3 4 ...\n $ Question_21           : num [1:1286] 4 3 5 4 4 3 4 4 2 2 ...\n $ Question_22           : num [1:1286] 2 2 5 3 4 2 4 4 2 3 ...\n $ Question_23           : num [1:1286] 4 5 5 4 4 3 5 4 3 2 ...\n $ FAS                   : chr [1:1286] \"CFA\" \"CFA\" \"CFA\" \"CFA\" ...\n\n\nSelecting numeric items only.\n\nlibrary(dplyr)\nCFA_items <- CFA %>% select(Question_1, Question_2, Question_3, Question_4, Question_5, Question_6, Question_7, Question_8, Question_9, Question_10, Question_11, Question_12, Question_13, Question_14, Question_15, Question_16, Question_17, Question_18, Question_19, Question_20, Question_21, Question_22, Question_23)\nstr(CFA_items)\n\ntibble [1,286 × 23] (S3: tbl_df/tbl/data.frame)\n $ Question_1 : num [1:1286] 4 3 2 3 3 2 2 3 2 2 ...\n $ Question_2 : num [1:1286] 1 1 2 2 1 1 1 1 1 1 ...\n $ Question_3 : num [1:1286] 1 1 3 3 1 2 3 2 3 4 ...\n $ Question_4 : num [1:1286] 4 5 4 4 4 3 3 3 2 2 ...\n $ Question_5 : num [1:1286] 4 4 2 4 2 3 4 4 3 3 ...\n $ Question_6 : num [1:1286] 2 2 3 4 2 3 2 1 1 2 ...\n $ Question_7 : num [1:1286] 2 2 5 4 5 3 5 2 1 1 ...\n $ Question_8 : num [1:1286] 2 2 3 3 1 2 2 3 1 1 ...\n $ Question_9 : num [1:1286] 1 1 4 2 1 1 4 5 1 5 ...\n $ Question_10: num [1:1286] 3 2 1 4 3 3 3 5 2 3 ...\n $ Question_11: num [1:1286] 2 2 1 3 3 2 3 2 1 1 ...\n $ Question_12: num [1:1286] 4 5 5 4 5 3 3 4 3 3 ...\n $ Question_13: num [1:1286] 2 1 4 4 3 3 4 2 1 1 ...\n $ Question_14: num [1:1286] 2 3 4 4 4 3 3 2 1 2 ...\n $ Question_15: num [1:1286] 2 1 2 4 2 3 5 3 3 4 ...\n $ Question_16: num [1:1286] 4 5 2 4 2 3 2 3 2 3 ...\n $ Question_17: num [1:1286] 3 2 4 4 3 3 2 3 2 2 ...\n $ Question_18: num [1:1286] 4 3 5 4 3 2 4 3 1 1 ...\n $ Question_19: num [1:1286] 2 1 5 2 3 2 2 4 4 5 ...\n $ Question_20: num [1:1286] 5 5 3 4 2 3 4 5 3 4 ...\n $ Question_21: num [1:1286] 4 3 5 4 4 3 4 4 2 2 ...\n $ Question_22: num [1:1286] 2 2 5 3 4 2 4 4 2 3 ...\n $ Question_23: num [1:1286] 4 5 5 4 4 3 5 4 3 2 ...\n\n\nCorrelating items.\n\nCFAMatrix <- cor(CFA_items)\ncored2 <- round (CFAMatrix, 2)\ncorrplot::corrplot(cored2)\n\n\n\n\nQ2 - 6. Conducting CFA based on the factor structures above. Also, get the path diagram, and model fit measures. 3. Click on ‘Factor 1’ and change it to the name that you decided for Factor 1 in the previous exercise. Drag the relevant items to the space below. 4. ‘Add New Factor’ and change the name to your decided name for Factor 2 in the previous exercise. Add the relevant items. 5. Do the same for the rest of the factors. 6. Under Additional output, tick Path diagram. Path diagram shows you a figure scheme of all observed and latent variables where observed variables load on their corresponding factors and the factors are correlated with each other.\n\nmodel <- \"\nFactor_1 =~ Question_1 + Question_21 + Question_4 + Question_16 + Question_12 + Question_20 + Question_5 + Question_3\nFactor_2 =~ Question_6 + Question_18 + Question_13 + Question_7 + Question_14 + Question_10\nFactor_3 =~ Question_8 + Question_11 + Question_17\nFactor_4 =~ Question_9 + Question_2 + Question_22\"\n\n\nfit <- lavaan::cfa(model, data=CFA_items)\nlavaan::parameterEstimates(fit)\n\n           lhs op         rhs    est    se       z pvalue ci.lower ci.upper\n1     Factor_1 =~  Question_1  1.000 0.000      NA     NA    1.000    1.000\n2     Factor_1 =~ Question_21  1.272 0.067  18.934      0    1.140    1.403\n3     Factor_1 =~  Question_4  1.158 0.063  18.342      0    1.034    1.281\n4     Factor_1 =~ Question_16  1.190 0.063  19.026      0    1.068    1.313\n5     Factor_1 =~ Question_12  1.180 0.062  18.955      0    1.058    1.302\n6     Factor_1 =~ Question_20  0.931 0.065  14.432      0    0.805    1.058\n7     Factor_1 =~  Question_5  1.023 0.063  16.318      0    0.900    1.146\n8     Factor_1 =~  Question_3 -1.267 0.071 -17.912      0   -1.405   -1.128\n9     Factor_2 =~  Question_6  1.000 0.000      NA     NA    1.000    1.000\n10    Factor_2 =~ Question_18  1.162 0.053  21.949      0    1.058    1.266\n11    Factor_2 =~ Question_13  0.934 0.046  20.221      0    0.844    1.025\n12    Factor_2 =~  Question_7  1.036 0.052  19.968      0    0.934    1.137\n13    Factor_2 =~ Question_14  0.934 0.047  19.819      0    0.841    1.026\n14    Factor_2 =~ Question_10  0.497 0.039  12.718      0    0.421    0.574\n15    Factor_3 =~  Question_8  1.000 0.000      NA     NA    1.000    1.000\n16    Factor_3 =~ Question_11  1.050 0.043  24.254      0    0.965    1.135\n17    Factor_3 =~ Question_17  1.068 0.044  24.482      0    0.982    1.153\n18    Factor_4 =~  Question_9  1.000 0.000      NA     NA    1.000    1.000\n19    Factor_4 =~  Question_2  0.632 0.069   9.193      0    0.497    0.766\n20    Factor_4 =~ Question_22  0.652 0.076   8.631      0    0.504    0.800\n21  Question_1 ~~  Question_1  0.449 0.019  23.191      0    0.411    0.486\n22 Question_21 ~~ Question_21  0.552 0.025  22.500      0    0.504    0.600\n23  Question_4 ~~  Question_4  0.529 0.023  22.894      0    0.484    0.575\n24 Question_16 ~~ Question_16  0.472 0.021  22.431      0    0.431    0.514\n25 Question_12 ~~ Question_12  0.472 0.021  22.484      0    0.431    0.514\n26 Question_20 ~~ Question_20  0.807 0.033  24.318      0    0.742    0.872\n27  Question_5 ~~  Question_5  0.654 0.027  23.806      0    0.600    0.707\n28  Question_3 ~~  Question_3  0.702 0.030  23.136      0    0.643    0.762\n29  Question_6 ~~  Question_6  0.752 0.033  22.647      0    0.687    0.817\n30 Question_18 ~~ Question_18  0.471 0.024  19.446      0    0.423    0.518\n31 Question_13 ~~ Question_13  0.493 0.023  21.734      0    0.449    0.537\n32  Question_7 ~~  Question_7  0.644 0.029  21.953      0    0.587    0.702\n33 Question_14 ~~ Question_14  0.542 0.025  22.072      0    0.494    0.590\n34 Question_10 ~~ Question_10  0.661 0.027  24.602      0    0.608    0.714\n35  Question_8 ~~  Question_8  0.345 0.018  18.829      0    0.309    0.381\n36 Question_11 ~~ Question_11  0.313 0.018  17.344      0    0.277    0.348\n37 Question_17 ~~ Question_17  0.299 0.018  16.692      0    0.264    0.334\n38  Question_9 ~~  Question_9  1.096 0.065  16.881      0    0.969    1.223\n39  Question_2 ~~  Question_2  0.477 0.027  17.576      0    0.424    0.530\n40 Question_22 ~~ Question_22  0.863 0.041  20.876      0    0.782    0.944\n41    Factor_1 ~~    Factor_1  0.260 0.023  11.303      0    0.215    0.305\n42    Factor_2 ~~    Factor_2  0.505 0.042  11.905      0    0.422    0.589\n43    Factor_3 ~~    Factor_3  0.414 0.029  14.106      0    0.357    0.472\n44    Factor_4 ~~    Factor_4  0.446 0.065   6.904      0    0.320    0.573\n45    Factor_1 ~~    Factor_2  0.302 0.021  14.147      0    0.260    0.344\n46    Factor_1 ~~    Factor_3  0.220 0.016  13.580      0    0.188    0.251\n47    Factor_1 ~~    Factor_4 -0.166 0.019  -8.669      0   -0.204   -0.129\n48    Factor_2 ~~    Factor_3  0.272 0.021  12.983      0    0.231    0.313\n49    Factor_2 ~~    Factor_4 -0.203 0.025  -8.033      0   -0.253   -0.153\n50    Factor_3 ~~    Factor_4 -0.078 0.020  -3.984      0   -0.116   -0.040\n\n\nQ7. Report Model Fit statistics for the four factor model. Do our findings meet the factor loading criteria?\nQ8. Compare the fit statistics of the four-factor model of the scale to a one-factor model (assuming no underlying factor solution for SAQ).\n\nmodel2 <- \"\nFactor_1 =~ Question_1 + Question_21 + Question_4 + Question_16 + Question_12 + Question_20 + Question_5 + Question_3 + Question_6 + Question_18 + Question_13 + Question_7 + Question_14 + Question_10 + Question_8 + Question_11 + Question_17 + Question_9 + Question_2 + Question_22\"\nfit2 <- lavaan::cfa(model2, data=CFA_items)\nlavaan::parameterEstimates(fit2)\n\n           lhs op         rhs    est    se       z pvalue ci.lower ci.upper\n1     Factor_1 =~  Question_1  1.000 0.000      NA     NA    1.000    1.000\n2     Factor_1 =~ Question_21  1.267 0.069  18.416      0    1.132    1.401\n3     Factor_1 =~  Question_4  1.147 0.065  17.775      0    1.020    1.273\n4     Factor_1 =~ Question_16  1.175 0.064  18.384      0    1.050    1.300\n5     Factor_1 =~ Question_12  1.209 0.064  18.811      0    1.083    1.334\n6     Factor_1 =~ Question_20  0.881 0.065  13.487      0    0.753    1.009\n7     Factor_1 =~  Question_5  1.020 0.064  15.919      0    0.894    1.146\n8     Factor_1 =~  Question_3 -1.229 0.072 -17.090      0   -1.370   -1.088\n9     Factor_1 =~  Question_6  1.212 0.075  16.164      0    1.065    1.359\n10    Factor_1 =~ Question_18  1.539 0.077  19.879      0    1.387    1.691\n11    Factor_1 =~ Question_13  1.285 0.068  18.872      0    1.151    1.418\n12    Factor_1 =~  Question_7  1.414 0.076  18.558      0    1.265    1.563\n13    Factor_1 =~ Question_14  1.283 0.069  18.513      0    1.147    1.419\n14    Factor_1 =~ Question_10  0.693 0.056  12.332      0    0.583    0.803\n15    Factor_1 =~  Question_8  0.845 0.057  14.814      0    0.733    0.957\n16    Factor_1 =~ Question_11  1.047 0.060  17.441      0    0.929    1.164\n17    Factor_1 =~ Question_17  1.076 0.060  17.794      0    0.958    1.195\n18    Factor_1 =~  Question_9 -0.534 0.075  -7.103      0   -0.682   -0.387\n19    Factor_1 =~  Question_2 -0.404 0.049  -8.192      0   -0.501   -0.308\n20    Factor_1 =~ Question_22 -0.539 0.063  -8.582      0   -0.662   -0.416\n21  Question_1 ~~  Question_1  0.465 0.019  23.968      0    0.427    0.503\n22 Question_21 ~~ Question_21  0.581 0.025  23.573      0    0.533    0.629\n23  Question_4 ~~  Question_4  0.557 0.023  23.833      0    0.511    0.603\n24 Question_16 ~~ Question_16  0.504 0.021  23.587      0    0.462    0.546\n25 Question_12 ~~ Question_12  0.478 0.020  23.382      0    0.438    0.518\n26 Question_20 ~~ Question_20  0.843 0.034  24.764      0    0.777    0.910\n27  Question_5 ~~  Question_5  0.672 0.028  24.358      0    0.618    0.726\n28  Question_3 ~~  Question_3  0.751 0.031  24.059      0    0.690    0.812\n29  Question_6 ~~  Question_6  0.899 0.037  24.303      0    0.827    0.972\n30 Question_18 ~~ Question_18  0.575 0.025  22.695      0    0.526    0.625\n31 Question_13 ~~ Question_13  0.532 0.023  23.350      0    0.487    0.576\n32  Question_7 ~~  Question_7  0.698 0.030  23.507      0    0.640    0.757\n33 Question_14 ~~ Question_14  0.581 0.025  23.529      0    0.533    0.630\n34 Question_10 ~~ Question_10  0.669 0.027  24.895      0    0.616    0.722\n35  Question_8 ~~  Question_8  0.585 0.024  24.570      0    0.539    0.632\n36 Question_11 ~~ Question_11  0.502 0.021  23.949      0    0.461    0.543\n37 Question_17 ~~ Question_17  0.488 0.021  23.826      0    0.448    0.529\n38  Question_9 ~~  Question_9  1.472 0.058  25.232      0    1.358    1.587\n39  Question_2 ~~  Question_2  0.615 0.024  25.186      0    0.567    0.663\n40 Question_22 ~~ Question_22  0.982 0.039  25.167      0    0.905    1.058\n41    Factor_1 ~~    Factor_1  0.244 0.022  11.075      0    0.201    0.287\n\n\nQ9. Report Model Fit statistics for the one-factor model. Do our findings meet the factor loading criteria?\nQ10. Comparing the 4-factor and the one-factor model, which one is a better fit? What is your conclusion from the CFA?"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Kia ora!\nWelcome to PSYC344.\nI am Usman Afzali, the course coordinator and lecturer of the course. I built this website during Jan, 2023 and will keep maintaining it in the future.\nThis website contains R code for statistics labs of the Year 3 Intermediate Research Methods and Statistics course taught at the School of Psychology, Speech and Hearing, University of Canterbury. The code is specific to the 2022 Summer course (2022-2023), but is very similar to the previous and hopefully future occurrences of the course.\nIn addition to statistics labs, some weekly introduction slides and the Course Outline are available too.\nFor the next occurrence, weekly lecture slides will be made available too.\n\n\n\n\n\n\nGitHub Repo\n\n\n\nPlease access the PSYC344 Github Repository to use or replicate the code for this website. You can also access the quaro or .qmd files from the repository.\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nFeel free to use/replicate the code.\nSuggestions welcome.\n\n\n\nKind regards.\nUsman Afzali, PhD\nwww.usmanafzali.com\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "intro.html#labs",
    "href": "intro.html#labs",
    "title": "Introduction",
    "section": "Labs",
    "text": "Labs\n\nLab 1 and 2\nThese labs are not included here as they don’t involve any data analysis. First, let’s learn how to use the available code for lab files.\n\n\nHow to use Lab Files\nEach of the lab files is a Quarto Markdwon or .qmd document. One way of using and replicating the code is to use the latest version of RStudio, at least Version: 2022.12.0+353 or later on your computer. Within RStudio, click on new file drop down menu and open a new Quarto Document (see the first figure), and name it. Once the .qmd document is opened, go to the source tab, erase the default code and paste the code from the corresponding lab file (from line 1 up to the final line). Then hit Render. You will see output in the Viewer menu. You can click Show in a new window to open rendered code in your local browser window (see the second figure for highlighted commands).\n\n\n\nRStudio Interface\n\n\n\n\n\nRStudio Interface 2\n\n\n\n\nLab 3: Descriptive statistics\nUse Lab3-Descriptive-Stats.qmd.\n\n\nLab 4: EFA and CFA\nUse Lab4-EFA-and-CFA.qmd."
  },
  {
    "objectID": "intro.html#slides",
    "href": "intro.html#slides",
    "title": "Introduction",
    "section": "Slides",
    "text": "Slides\nSlides are made using revealjs on Quarto. The process of using them is similar as using the other .qmd documents explained above. Once they are opened in RStudio, the Present button from the Presentation tab can be used to open the presentation in the local browser - that can be presented from there.\n\nIntro to Week 9\nUse Intro-to-Week9.qmd."
  },
  {
    "objectID": "labs.html",
    "href": "labs.html",
    "title": "Labs",
    "section": "",
    "text": "Lab 5: Hypothesis testing with t-tests\n\n\n\n\n\n\n\n\n\n\n\n\nUsman Afzali and Bethany Growns\n\n\n\n\n\n\n\n\nLab 3 - Descriptive Statisitcs\n\n\n\n\n\n\n\n\n\n\n\n\nUsman Afzali\n\n\n\n\n\n\n\n\nLab 4: Exploratory and Confirmatory Factor Analyses\n\n\n\n\n\n\n\n\n\n\n\n\nUsman Afzali\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "presentations/week09/Intro-to-Week9.html",
    "href": "presentations/week09/Intro-to-Week9.html",
    "title": "Intro to Week 9",
    "section": "",
    "text": "Happening This Week"
  },
  {
    "objectID": "presentations/week09/Intro-to-Week9.html#happening-this-week",
    "href": "presentations/week09/Intro-to-Week9.html#happening-this-week",
    "title": "Intro to Week 9",
    "section": "Happening This Week",
    "text": "Happening This Week\n\n\nLectures - Regression\nLecture 26 has been divided into three parts.\nLab: ANOVA\nLecture Quiz"
  },
  {
    "objectID": "presentations/week12/Intro-to-Week12.html",
    "href": "presentations/week12/Intro-to-Week12.html",
    "title": "Intro to Week 12",
    "section": "",
    "text": "Happening This Week\nPG Options\n\n\n\n\n\n\nLectures - Qualitative Methods\nDelivered by Dr Fleur Pawsey\nLab: Regression - The final lab of the semester\nLecture Quiz\n\n\n\n\n\n\n\nRecording from A/Prof Ewald Neumann\nThe new course: PSYC480\nIf you are a prospective Honours/PG student"
  },
  {
    "objectID": "presentations/week10/Intro-to-Week10.html",
    "href": "presentations/week10/Intro-to-Week10.html",
    "title": "Intro to Week 10",
    "section": "",
    "text": "Happening This Week\nMisc\nPG Options\n\n\n\n\n\n\nLectures - Qualitative Methods\nDelivered by Dr Fleur Pawsey\nLab: Regression - The final lab of the semester\nLecture Quiz\n\n\n\n\n\n\n\nCheck Gradebook\nMy availability\n\n\n\n\n\n\n\nRecording from A/Prof Ewald Neumann\nThe new course: PSYC480\nOn campus MATLAB crash course\nIf you are a prospective Honours/PG student"
  },
  {
    "objectID": "presentations/week11/Intro-to-Week11.html",
    "href": "presentations/week11/Intro-to-Week11.html",
    "title": "Intro to Week 11",
    "section": "",
    "text": "Happening This Week\nPG Options\n\n\n\n\n\n\nLectures - Qualitative Methods\nDelivered by Dr Fleur Pawsey\nLab: Regression - The final lab of the semester\nLecture Quiz\n\n\n\n\n\n\n\nRecording from A/Prof Ewald Neumann\nThe new course: PSYC480\nIf you are a prospective Honours/PG student"
  },
  {
    "objectID": "presentations.html",
    "href": "presentations.html",
    "title": "Presentataions",
    "section": "",
    "text": "Intro to Week 9\n\n\nPSYC344\n\n\n\n\n\n\n\n\n\nFeb 5, 2023\n\n\nDr Usman Afzali\n\n\n\n\n\n\n\n\nIntro to Week 12\n\n\nPSYC344\n\n\n\n\n\n\n\n\n\nFeb 5, 2023\n\n\nDr Usman Afzali\n\n\n\n\n\n\n\n\nExam Guidelines\n\n\nPSYC344\n\n\n\n\n\n\n\n\n\nFeb 5, 2023\n\n\nDr Usman Afzali\n\n\n\n\n\n\n\n\nIntro to Week 10\n\n\nPSYC344\n\n\n\n\n\n\n\n\n\nFeb 5, 2023\n\n\nDr Usman Afzali\n\n\n\n\n\n\n\n\nIntro to Week 11\n\n\nPSYC344\n\n\n\n\n\n\n\n\n\nFeb 5, 2023\n\n\nDr Usman Afzali\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "presentations/week09/Intro-to-Week9.html#kauhau",
    "href": "presentations/week09/Intro-to-Week9.html#kauhau",
    "title": "Intro to Week 9",
    "section": "Kauhau",
    "text": "Kauhau\n\n\nHappening This Week"
  },
  {
    "objectID": "presentations/week10/Intro-to-Week10.html#happening-this-week",
    "href": "presentations/week10/Intro-to-Week10.html#happening-this-week",
    "title": "Intro to Week 10",
    "section": "Happening This Week",
    "text": "Happening This Week\n\n\nLectures - Qualitative Methods\nDelivered by Dr Fleur Pawsey\nLab: Regression - The final lab of the semester\nLecture Quiz"
  },
  {
    "objectID": "presentations/week10/Intro-to-Week10.html#misc",
    "href": "presentations/week10/Intro-to-Week10.html#misc",
    "title": "Intro to Week 10",
    "section": "Misc",
    "text": "Misc\n\n\nCheck Gradebook\nMy availability"
  },
  {
    "objectID": "presentations/week10/Intro-to-Week10.html#pg-options",
    "href": "presentations/week10/Intro-to-Week10.html#pg-options",
    "title": "Intro to Week 10",
    "section": "PG Options",
    "text": "PG Options\n\n\nRecording from A/Prof Ewald Neumann\nThe new course: PSYC480\nOn campus MATLAB crash course\nIf you are a prospective Honours/PG student"
  },
  {
    "objectID": "presentations/week11/Intro-to-Week11.html#happening-this-week",
    "href": "presentations/week11/Intro-to-Week11.html#happening-this-week",
    "title": "Intro to Week 11",
    "section": "Happening This Week",
    "text": "Happening This Week\n\n\nLecture 30 Open Science and preregistration by Andy Vonasch\nLecture 31 Power, Effect Size, and the Replication Crisis by Usman Afzali (at the beginning of this lecture, there are some updates re Lab Report)\nLecture 32 The New Statistics and robust statistics by Usman Afzali\nRequired readings\nLecture Quiz\nLab Report submission"
  },
  {
    "objectID": "presentations/week11/Intro-to-Week11.html#pg-options",
    "href": "presentations/week11/Intro-to-Week11.html#pg-options",
    "title": "Intro to Week 11",
    "section": "PG Options",
    "text": "PG Options\n\n\nRecording from A/Prof Ewald Neumann\nThe new course: PSYC480\nIf you are a prospective Honours/PG student"
  },
  {
    "objectID": "presentations/week12/Intro-to-Week12.html#happening-this-week",
    "href": "presentations/week12/Intro-to-Week12.html#happening-this-week",
    "title": "Intro to Week 12",
    "section": "Happening This Week",
    "text": "Happening This Week\n\n\nLecture - Science communication\nLecture - Panel discussion\nLecture - Kahoot review\nLearning activity: Pulse survey\nLecture Quiz"
  },
  {
    "objectID": "presentations/week12/Intro-to-Week12.html#pg-options",
    "href": "presentations/week12/Intro-to-Week12.html#pg-options",
    "title": "Intro to Week 12",
    "section": "PG Options",
    "text": "PG Options\n\n\nRecording from A/Prof Ewald Neumann\nThe new course: PSYC480\nIf you are a prospective Honours/PG student"
  },
  {
    "objectID": "presentations/week11/Intro-to-Week11.html#exam-related",
    "href": "presentations/week11/Intro-to-Week11.html#exam-related",
    "title": "Intro to Week 11",
    "section": "Exam related",
    "text": "Exam related\n\n\nPractice exam TBA by the 1st Feb\nExam instructions (video) TBA in the assessments section by the 1st Feb"
  },
  {
    "objectID": "presentations/week11/Intro-to-Week11.html#finally",
    "href": "presentations/week11/Intro-to-Week11.html#finally",
    "title": "Intro to Week 11",
    "section": "Finally",
    "text": "Finally\nIf you have any questions, please reach out"
  },
  {
    "objectID": "presentations/Exam prep/exam-prep.html#final-exam",
    "href": "presentations/Exam prep/exam-prep.html#final-exam",
    "title": "Exam Guidelines",
    "section": "Final Exam",
    "text": "Final Exam\n\n\nDate and time: 13 Feb 2023 at 10:00am\nDuration: 2 hours\nStyle: Open book. You may use any written, printed, or electronic materials. However, you must not collaborate or communicate with anyone else regarding the contents of the exam.\nCalculator: Not needed/not permitted\nInvigilation: Via Zoom\nMedium: Learn quiz activity"
  },
  {
    "objectID": "presentations/Exam prep/exam-prep.html#guidelines",
    "href": "presentations/Exam prep/exam-prep.html#guidelines",
    "title": "Exam Guidelines",
    "section": "Guidelines",
    "text": "Guidelines\nPlease read the online invigilation guidelines on LEARN in the “Aromatawai | Assessment” section under the final exam heading\n\n\nMake sure you are familiar with your computer set-up and its necessary software (Zoom, internet). You need to have a working webcam and mic\nArrange for a well-lit quiet space to complete your exam. No one can be in the room with you. Ensure that your internet will be working during the exam."
  },
  {
    "objectID": "presentations/Exam prep/exam-prep.html#studying-for-an-open-book-exam-from-dr-jen-wong",
    "href": "presentations/Exam prep/exam-prep.html#studying-for-an-open-book-exam-from-dr-jen-wong",
    "title": "Exam Guidelines",
    "section": "Studying for an Open-book Exam (from Dr Jen Wong)",
    "text": "Studying for an Open-book Exam (from Dr Jen Wong)\n\n\nThere will be 40 multiple choice and 5 short answer questions on the exam. Within a 2-hour timeframe, you will not have enough time to search and consult your lecture notes for each question.\nIt can help to organise your lecture notes, so you know where to find information if you do need to refer to it"
  },
  {
    "objectID": "presentations/Exam prep/exam-prep.html#studying-for-an-open-book-exam-from-dr-jen-wong-1",
    "href": "presentations/Exam prep/exam-prep.html#studying-for-an-open-book-exam-from-dr-jen-wong-1",
    "title": "Exam Guidelines",
    "section": "Studying for an Open-book Exam (from Dr Jen Wong)",
    "text": "Studying for an Open-book Exam (from Dr Jen Wong)\n\n\nYou still need to study for an open-book exam! Most questions will assess your understanding, not memorization of PSYC344 concepts\nCreate your own notes"
  },
  {
    "objectID": "presentations/Exam prep/exam-prep.html#guidelines-1",
    "href": "presentations/Exam prep/exam-prep.html#guidelines-1",
    "title": "Exam Guidelines",
    "section": "Guidelines",
    "text": "Guidelines\n\nYou must log into your designated Zoom 20-30 mins prior to your exam so your invigilator can check your photo ID. Use the bathroom beforehand!"
  },
  {
    "objectID": "presentations/Exam prep/exam-prep.html#studying-for-an-open-book-exam-from-dr-jen-wong-2",
    "href": "presentations/Exam prep/exam-prep.html#studying-for-an-open-book-exam-from-dr-jen-wong-2",
    "title": "Exam Guidelines",
    "section": "Studying for an Open-book Exam (from Dr Jen Wong)",
    "text": "Studying for an Open-book Exam (from Dr Jen Wong)\nResources:\n\nWeekly lecture quizzes with specific or general feedback now available\nAndy Field’s MC practice questions: https://edge.sagepub.com/field5e/multiple-choice-quiz"
  },
  {
    "objectID": "presentations/Exam prep/exam-prep.html#studying-for-an-open-book-exam-from-dr-jen-wong-3",
    "href": "presentations/Exam prep/exam-prep.html#studying-for-an-open-book-exam-from-dr-jen-wong-3",
    "title": "Exam Guidelines",
    "section": "Studying for an Open-book Exam (from Dr Jen Wong)",
    "text": "Studying for an Open-book Exam (from Dr Jen Wong)\nOnline guides:\n\nCreating notes, conceptual mind maps\nhttps://www.lib.sfu.ca/about/branches-depts/slc/learning/exam-types/open-book-exams\nhttps://www.trentu.ca/academicskills/how-guides/how-study/prepare-and-write-exams/preparing-online-open-book-exam"
  },
  {
    "objectID": "presentations/Exam prep/exam-prep.html#studying-for-an-open-book-exam-from-dr-jen-wong-4",
    "href": "presentations/Exam prep/exam-prep.html#studying-for-an-open-book-exam-from-dr-jen-wong-4",
    "title": "Exam Guidelines",
    "section": "Studying for an Open-book Exam (from Dr Jen Wong)",
    "text": "Studying for an Open-book Exam (from Dr Jen Wong)\nTest-taking strategy\n\nhttps://www.educationcorner.com/openbook-tests.html\nhttps://www.student.unsw.edu.au/preparing-open-book-exams"
  },
  {
    "objectID": "presentations/week12/Intro-to-Week12.html#course-wrap-up",
    "href": "presentations/week12/Intro-to-Week12.html#course-wrap-up",
    "title": "Intro to Week 12",
    "section": "Course wrap up",
    "text": "Course wrap up\n\n\nThe motive behind lectures\nIt might be difficult for you to assess how much we learned unless we use strength-based approach - 206, 344, 460\nHow does PG work?"
  },
  {
    "objectID": "how-to-use.html",
    "href": "how-to-use.html",
    "title": "How to use this website?",
    "section": "",
    "text": "First, let’s learn how to use the available code for lab files.\n\n\nThe R code in this website is executable. Each of the lab files is a Quarto Markdwon or .qmd document.\nIf you have background in coding and are just looking for a piece of code, or if you want to browse the code, you can look at the Labs section and click on the relevant lab. The relevant code can be ran on R scrip, R markdown or Quarto markdown.\nIf you want to access the very .qmd documents to use and replicate the code, you should:\n\nUse the latest version of RStudio, at least Version: 2022.12.0+353 or later on your computer.\nWithin RStudio, click on new file drop down menu and open a new Quarto Document (see the first figure), and name it.\nOnce the .qmd document is opened, go to the source tab, erase the default code and paste the code from the corresponding lab file (from line 1 up to the final line). Then hit Render. You will see output in the Viewer menu.\nYou can click Show in a new window to open rendered code in your local browser window (see the second figure for highlighted commands).\n\n\n\n\nRStudio Interface\n\n\n\n\n\nRStudio Interface 2\n\n\n\n\n\n\n\n\nRepo\n\n\n\nLab files are within the labs folder in Github Repo.\n\n\n\n\n\nThese labs are not included as they don’t involve any data analysis.\n\n\n\nUse the file Lab3-Descriptive-Stats.qmd.\n\n\n\nUse Lab4-EFA-and-CFA.qmd.\n\n\n\nUse Lab5-t-test.qmd.\n\n\n\n\n\n\nMore to come\n\n\n\nFurther lab files are yet to be added."
  },
  {
    "objectID": "how-to-use.html#slides",
    "href": "how-to-use.html#slides",
    "title": "How to use this website?",
    "section": "Slides",
    "text": "Slides\nSlides are made using revealjs on Quarto. There are three ways to use them:\n\nUse live: From Presentations, click the corresponding lab. They can be used live simply by clicking on them and using the keyboard arrow keys.\nSave as pdf: Get to the corresponding lab as explained above. This option only works with Google Chrome as explained here.\nThe revealjs files: The original code files on Github Repo are in the presentations folder.\n\nThe process of using them is similar as using any other .qmd documents explained above. Once they are opened in RStudio, the Present button from the Presentation tab can be used to open the presentation in the local browser - that can be presented from there.\n\nIntro to Week 9\nUse Intro-to-Week9.qmd.\n\n\nIntro to Week 10\nUse Intro-to-Week10.qmd.\n\n\nIntro to Week 11\nUse Intro-to-Week11.qmd.\n\n\nIntro to Week 12\nUse Intro-to-Week12.qmd.\n\n\nExam guidelines\nUse exam-guidelines.qmd."
  },
  {
    "objectID": "how-to-use.html#github-repo",
    "href": "how-to-use.html#github-repo",
    "title": "How to use this website?",
    "section": "GitHub Repo",
    "text": "GitHub Repo\nPlease access the PSYC344 Github Repository to use or replicate the code for this website. You can also access the quaro or .qmd files from the repository.\n\nLab 1 and 2\nThese labs are not included as they don’t involve any data analysis.\n\n\nLab 3: Descriptive statistics\nUse the file Lab3-Descriptive-Stats.qmd.\n\n\nLab 4: EFA and CFA\nUse Lab4-EFA-and-CFA.qmd."
  }
]